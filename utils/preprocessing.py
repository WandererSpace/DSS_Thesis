from __future__ import annotations
from pathlib import Path
import re, json
import numpy as np
import pandas as pd
import pyreadstat
from .config import load_config, abspath, ensure_dirs

ALIASES = {  # å¸¸è§å­—æ®µåˆ«åå…¼å®¹
    "n_age_dv": "n_dvage",
}

def _resolve_keep_cols(keep_cols, available):
    resolved = []
    for c in keep_cols:
        if c in available:
            resolved.append(c)
        elif c in ALIASES and ALIASES[c] in available:
            resolved.append(ALIASES[c])
    # å»é‡ä¿åº
    seen=set(); out=[]
    for c in resolved:
        if c not in seen:
            seen.add(c); out.append(c)
    return out

def load_candidate_vars(cfg, root: Path) -> list[str]:
    manifest = abspath(root, cfg["ukhls"]["candidate_vars_csv"])
    df = pd.read_csv(manifest)
    cols = df["varname"].astype(str).tolist()
    # å¸¸ç”¨å…ƒæ•°æ®åˆ—å…œåº•åŠ å…¥
    for must in ["pidp","n_hidp","n_pno", cfg["processing"]["sample_flag_col"]]:
        if must not in cols:
            cols.append(must)
    return cols

def read_indresp_selected(cfg=None, root: Path | None=None) -> pd.DataFrame:
    """Robustly read the subset of variables that actually exist in the .dta file."""
    cfg, root = load_config(root)
    dta_path = abspath(root, cfg["ukhls"]["indresp_dta"])

    # 1. è¯»å–å€™é€‰å˜é‡æ¸…å•
    manifest = abspath(root, cfg["ukhls"]["candidate_vars_csv"])
    df_manifest = pd.read_csv(manifest)
    keep_cols = df_manifest["varname"].astype(str).unique().tolist()

    # 2. å®‰å…¨è·å– .dta ä¸­çš„çœŸå®åˆ—åï¼ˆå…¼å®¹æ—§ pandasï¼‰
    try:
        reader = pd.read_stata(dta_path, iterator=True)
        preview = reader.read(1)
        available_cols = list(preview.columns)
        reader.close()
    except Exception:
        # å¦‚æœ iterator å¤±è´¥ï¼Œå°±ç›´æ¥å…¨è¯»ä¸€éï¼ˆæœ€æ…¢ä½†æœ€å®‰å…¨ï¼‰
        preview = pd.read_stata(dta_path)
        available_cols = list(preview.columns)

    # 3. åªä¿ç•™ç¡®å®å­˜åœ¨çš„åˆ—
    usecols = [c for c in keep_cols if c in available_cols]

    # 4. çœŸæ­£è¯»å–éœ€è¦çš„å­—æ®µ
    df = pd.read_stata(dta_path, columns=usecols)
    return df

def normalize_missing(df: pd.DataFrame, neg_codes: list[int]) -> pd.DataFrame:
    NEG = set(neg_codes)
    for c in df.columns:
        if pd.api.types.is_numeric_dtype(df[c]):
            df[c] = df[c].astype("float64")
            df[c] = df[c].where(~df[c].isin(NEG), np.nan)
    return df

def engineer_employment_history_features(df: pd.DataFrame) -> pd.DataFrame:
    import numpy as np
    import re

    # å¼ºåˆ¶æ•°å€¼åŒ–ï¼šå°±ä¸šç‰‡æ®µè®¡æ•°åˆ—
    for col in ["n_nmpsp_dv", "n_nnmpsp_dv", "n_nunmpsp_dv"]:
        if col in df.columns:
            df[col] = df[col].astype(str).replace("nan", np.nan)
            df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).astype(float)

    # ç»“æŸåŸå› ï¼ˆè£å‘˜/è§£é›‡/åˆåŒæœŸæ»¡â€¦/COVIDï¼‰
    end_cols  = [c for c in df.columns if re.match(r"n_reasendoth\d+_code", c)]
    next_cols = [c for c in df.columns if re.match(r"n_nextelse\d+", c)]

    def _count_codes(row, cols, codes):
        cnt = 0
        for c in cols:
            v = row.get(c)
            if pd.notna(v):
                try:
                    if int(v) in codes:
                        cnt += 1
                except Exception:
                    pass
        return cnt

    if end_cols:
        df["end_involuntary_cnt"] = df.apply(lambda r: _count_codes(r, end_cols, {3,4,5,26,27,28}), axis=1)
        df["end_covid_cnt"]       = df.apply(lambda r: _count_codes(r, end_cols, {23}), axis=1)
    else:
        df["end_involuntary_cnt"] = 0
        df["end_covid_cnt"] = 0

    if next_cols:
        df["next_unemp_cnt"] = df.apply(lambda r: _count_codes(r, next_cols, {1}), axis=1)
    else:
        df["next_unemp_cnt"] = 0

    return df

def build_targets(df: pd.DataFrame, cfg) -> pd.DataFrame:
    """
    æ„å»ºåˆ†ç±»ç›®æ ‡å˜é‡ target_clsï¼ŒåŸºäº GHQ caseness æŒ‡æ ‡ï¼šn_scghq2_dvã€‚
    ä½¿ç”¨é˜ˆå€¼ï¼ˆé»˜è®¤ 4+ï¼‰åˆ’åˆ†å¿ƒç†å›°æ‰°çŠ¶æ€ã€‚

    æ³¨æ„ï¼šä¸å†ä½¿ç”¨ n_scflag_dv è¿›è¡Œæ ·æœ¬ç­›é€‰ï¼Œç›´æ¥ä»¥ç›®æ ‡å­—æ®µå€¼æ˜¯å¦æœ‰æ•ˆä¸ºæ ‡å‡†ã€‚
    """

    target_col = cfg["processing"]["classification_target"]
    threshold = cfg["processing"]["classification_threshold"]

    # ä¿ç•™æœ‰æ•ˆ GHQ å€¼ï¼ˆ0~12ï¼‰ï¼Œå»é™¤éæ³•ç¼–ç  -9/-8 ç­‰
    df = df[df[target_col].between(0, 12)]

    # åˆ›å»ºäºŒåˆ†ç±»ç›®æ ‡åˆ—
    df.loc[:, "target_cls"] = (df[target_col] >= threshold).astype("Int64")

    return df

def run_basic_processing_and_save(cfg=None, root: Path | None=None):
    cfg, root = load_config(root)
    paths = cfg["paths"]; proc = cfg["processing"]

    out_sel  = abspath(root, proc["outputs"]["indresp_selected"])
    out_feat = abspath(root, proc["outputs"]["features"])
    out_ds   = abspath(root, proc["outputs"]["dataset_for_model"])
    ensure_dirs(out_sel.parent, out_feat.parent, out_ds.parent)

    df = read_indresp_selected(cfg, root)
    df = normalize_missing(df, proc["negative_missing_codes"])
    # â¬‡ï¸ æ·»åŠ è°ƒè¯•è¡Œ
    print("target_cls existence:", "target_cls" in df.columns)

    df = engineer_employment_history_features(df)
    df = build_targets(df, cfg)
    # ğŸš« æ¶ˆé™¤ category ç±»å‹ï¼ˆå¦åˆ™ to_parquet/pyarrow ä¼šå‡ºé”™ï¼‰
    for col in df.select_dtypes(include="category").columns:
        df[col] = df[col].astype(str)

    # ä¿å­˜
    df.to_parquet(out_sel, index=False)
    # å¯æŒ‰éœ€åœ¨æ­¤å¤„åšè¿›ä¸€æ­¥ç‰¹å¾å¤„ç†åå¦å­˜ä¸º out_feat/out_ds
    df.to_parquet(out_feat, index=False)
    df.to_parquet(out_ds, index=False)
    print("Final processed dataset shape:", df.shape)
    print("target_cls value counts:", df["target_cls"].value_counts(dropna=False) if "target_cls" in df.columns else "No target_cls")
    return df, out_ds
    