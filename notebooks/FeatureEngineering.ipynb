{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "953883dc-fae3-476f-9928-942a6f7ba077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91d7feb6-28d8-4596-a460-07807dda279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "# Set font for better compatibility (adjust if specific font is needed)\n",
    "plt.rcParams['figure.figsize'] = (15, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ec717be-8654-4d65-b1c7-b26d67c711ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Configuration\n",
    "# ------------------------------------\n",
    "processed_dir = \"/Users/wanderer/DSS_Thesis/data/processed\"\n",
    "file_name = \"n_indresp_cleaned_02.csv\"\n",
    "file_path = os.path.join(processed_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65feb70e-75b1-40eb-b3c4-309a704c9514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from: /Users/wanderer/DSS_Thesis/data/processed/n_indresp_cleaned_02.csv\n"
     ]
    }
   ],
   "source": [
    "# --- GHQ Items List and UKHLS Missing Codes ---\n",
    "UKHLS_MISSING_CODES = [-9.0, -8.0, -7.0, -2.0, -1.0]\n",
    "\n",
    "# --- Helper Function 1: Clean Missing Codes (for float conversion) ---\n",
    "def clean_missing_codes(df, cols):\n",
    "    \"\"\"Replaces UKHLS special negative codes with NaN and ensures float type.\"\"\"\n",
    "    # Note: Use errors='ignore' in astype(float) if the column might contain non-numeric strings\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(UKHLS_MISSING_CODES, np.nan).astype(float, errors='ignore')\n",
    "    return df\n",
    "\n",
    "# --- Helper Function 2: Clean and Convert to Nullable Integer ---\n",
    "def clean_and_convert_to_nullable_int(df, cols):\n",
    "    \"\"\"Converts UKHLS missing codes to NaN and converts to Pandas' nullable Int64.\"\"\"\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(UKHLS_MISSING_CODES, np.nan)\n",
    "            try:\n",
    "                # Convert to nullable Int64 (supports NaN)\n",
    "                df[col] = df[col].astype('Int64')\n",
    "            except Exception:\n",
    "                # Fallback to float if conversion to Int64 fails\n",
    "                df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "def clean_and_convert_to_nullable_float(df, columns):\n",
    "    \"\"\"\n",
    "    Replace UKHLS-style special missing codes (-9, -8, -7, -2, -1) with np.nan\n",
    "    and convert columns to nullable float dtype.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace([-9, -8, -7, -2, -1], np.nan)\n",
    "            df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 0. Load Data and Initial Cleaning\n",
    "# ==============================================================================\n",
    "try:\n",
    "    df = pd.read_csv(file_path) \n",
    "    print(f\"Successfully loaded data from: {file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}. Please check the path and filename.\")\n",
    "    exit() \n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during file loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Apply base cleaning to all columns that might contain UKHLS codes\n",
    "all_cols_to_clean = [col for col in df.columns if col not in ['pidp', 'pid', 'n_hidp']]\n",
    "df = clean_missing_codes(df, all_cols_to_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8ce4ab8-8c75-406b-8b38-23e6fd80a872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['primary_mh_indicator'] = df[primary_mh_vars].max(axis=1, skipna=True)\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['secondary_mh_indicator'] = df[secondary_mh_vars_no_none].max(axis=1, skipna=True)\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['any_mh_problem'] = df[all_positive_mh_vars].max(axis=1, skipna=True)\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mental_health_status'] = np.nan # Default to NaN\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:34: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Associated MH Issue' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['primary_mh_indicator'] == 1.0,\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['first_job_duration_months'] = np.where(\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['early_career_unemp_risk'] = np.nan\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Never Employed' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['n_j1none'] == 2, 'early_career_unemp_risk'] = 'Never Employed'\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['total_spells_since_last_int'] = df['n_nnmpsp_dv'] + df['n_nmpsp_dv']\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['employment_stability_level'] = np.nan\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:116: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'High Stability: No change reported' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['total_spells_since_last_int'] == 0,\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:134: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['unemployment_spells_count'] = df['n_nunmpsp_dv']\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ghq12_continuous_score'] = df[ghq_caseness_var].astype(float)\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['education_level_revised'] = df[isced_var].map(education_mapping_revised).fillna('Missing/Inapplicable')\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['education_level'] = df['education_level_revised']\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:204: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['all_btec_nan'] = df[vocational_vars].isnull().all(axis=1)\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:208: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['has_vocational_qual'] = df[vocational_vars].max(axis=1, skipna=True)\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:233: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['female'] = 0\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:243: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['has_disability'] = 0\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_67833/1751457351.py:253: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['financial_difficulty'] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Engineering 1: Mental Health Status Completed ---\n",
      "mental_health_status\n",
      "Missing/Inapplicable    26833\n",
      "No Reported MH Issue     6229\n",
      "Associated MH Issue      2054\n",
      "Other MH Issue            355\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Feature Engineering 2: Early Career Unemployment Risk Completed ---\n",
      "early_career_unemp_risk\n",
      "Missing/Inapplicable                29180\n",
      "Stable Start: Left Long-Term Job     4078\n",
      "Early Failure: Quick Exit            1171\n",
      "Stable Start: Still in First Job      691\n",
      "Never Employed                        275\n",
      "Extended Unemp after Quick Exit        76\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Feature Engineering 3: Employment Stability Completed ---\n",
      "employment_stability_level\n",
      "Missing/Inapplicable                           14142\n",
      "Moderate Stability: Single Employment Spell    12422\n",
      "Low Stability: Non-Employment Spells Only       8269\n",
      "High Stability: No change reported               606\n",
      "High Volatility: Frequent Changes                 32\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Feature Engineering A: GHQ-12 Score (Aggregated) Completed ---\n",
      "--- Feature Engineering B: Education Level (Ordered) REVISED: Non-High vs. High Created ---\n",
      "\n",
      "--- Feature Engineering B.1: Vocational Qualification Indicator Completed ---\n",
      "has_vocational_qual\n",
      "<NA>    34680\n",
      "1         791\n",
      "Name: count, dtype: Int64\n",
      "--- Feature Engineering C: Control Variables (Binary/OHE) Completed ---\n",
      "\n",
      "--- Feature Engineering D: Advanced Employment Features (Fixed) Completed ---\n",
      "       is_currently_unemployed  last_unemployment_duration_months  \\\n",
      "count                  35341.0                        2305.000000   \n",
      "mean                  0.042132                         130.735792   \n",
      "std                   0.200894                         126.760077   \n",
      "min                        0.0                           0.000000   \n",
      "25%                        0.0                          35.000000   \n",
      "50%                        0.0                          95.000000   \n",
      "75%                        0.0                         188.000000   \n",
      "max                        1.0                         836.000000   \n",
      "\n",
      "       marginally_attached  \n",
      "count              14600.0  \n",
      "mean              0.168356  \n",
      "std               0.374195  \n",
      "min                    0.0  \n",
      "25%                    0.0  \n",
      "50%                    0.0  \n",
      "75%                    0.0  \n",
      "max                    1.0  \n",
      "\n",
      "--- Feature Engineering E: Demographic and Control Variables (Fixed) Completed ---\n",
      "\n",
      "--- Feature Engineering E: Demographic and Control Variables Completed ---\n",
      "\n",
      "--- Feature Engineering F: Previous Wave MH Indicators Completed ---\n",
      "\n",
      "--- Feature Engineering G: Loneliness/Social Isolation Index Completed ---\n",
      "\n",
      "--- Feature Engineering H: Health Behavior (Smoking Status) Completed ---\n",
      "\n",
      "--- Feature Engineering I: Objective Income and Economic Pressure Completed ---\n",
      "\n",
      "--- Feature Engineering J: Socioeconomic Class (NS-SEC) Completed ---\n",
      "\n",
      "--- Feature Engineering K: Job Exit Reason Indicators Completed ---\n",
      "\n",
      "--- Feature Engineering L: Geographic Region Dummies Completed ---\n",
      "\n",
      "--- ALL Feature Engineering Stages Complete. Optimized data saved to: /Users/wanderer/DSS_Thesis/data/processed/n_indresp_FINAL_FEATURES_OPTIMIZED02.csv ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. Feature Engineering: Mental Health Status (Outcome Variable) - USER CODE\n",
    "# ==============================================================================\n",
    "\n",
    "# 1.1 Define Primary (Associated) and Secondary (Other) MH Variable Groups\n",
    "primary_mh_vars = [\n",
    "    'n_mhcond19', 'n_mhcond2', 'n_mhcond5', 'n_mhcond13', \n",
    "    'n_mhcond17', 'n_mhcond18', 'n_mhcond97'\n",
    "]\n",
    "secondary_mh_vars_no_none = [ # Exclude n_mhcond96 from the positive check for now\n",
    "    'n_mhcond8', 'n_mhcond9', 'n_mhcond6', 'n_mhcond10', \n",
    "    'n_mhcond4', 'n_mhcond11', 'n_mhcond12', 'n_mhcond14', \n",
    "    'n_mhcond3', 'n_mhcond15', 'n_mhcond16'\n",
    "]\n",
    "all_positive_mh_vars = primary_mh_vars + secondary_mh_vars_no_none\n",
    "all_mh_vars_including_96 = all_positive_mh_vars + ['n_mhcond96']\n",
    "\n",
    "# 1.2 Clean and Prepare MH Data\n",
    "# Convert UKHLS missing codes to NaN and ensure float type for calculation.\n",
    "# This cleaning is handled by the initial clean_missing_codes call, but we ensure float type.\n",
    "df[all_mh_vars_including_96] = df[all_mh_vars_including_96].astype(float)\n",
    "\n",
    "# 1.3 Create Indicator Variables (1.0 = Yes mentioned, NaN = Missing, 0.0 = Not mentioned)\n",
    "df['primary_mh_indicator'] = df[primary_mh_vars].max(axis=1, skipna=True)\n",
    "df['secondary_mh_indicator'] = df[secondary_mh_vars_no_none].max(axis=1, skipna=True)\n",
    "df['any_mh_problem'] = df[all_positive_mh_vars].max(axis=1, skipna=True)\n",
    "\n",
    "# 1.4 Construct the Final Categorical Variable 'mental_health_status'\n",
    "df['mental_health_status'] = np.nan # Default to NaN\n",
    "\n",
    "# --- Prioritization Logic ---\n",
    "\n",
    "# 1. Associated MH Issue (any primary condition is 1.0)\n",
    "df.loc[df['primary_mh_indicator'] == 1.0, \n",
    "       'mental_health_status'] = 'Associated MH Issue'\n",
    "\n",
    "# 2. Other MH Issue (secondary condition is 1.0 AND no primary condition is 1.0)\n",
    "df.loc[(df['primary_mh_indicator'] == 0.0) & (df['secondary_mh_indicator'] == 1.0), \n",
    "       'mental_health_status'] = 'Other MH Issue'\n",
    "\n",
    "# 3. No Reported MH Issue (Max of all positive MH vars must be 0.0)\n",
    "mh_answered_but_no_problem = (df['any_mh_problem'] == 0.0)\n",
    "df.loc[mh_answered_but_no_problem, \n",
    "       'mental_health_status'] = 'No Reported MH Issue'\n",
    "\n",
    "# 4. Missing/Inapplicable \n",
    "df['mental_health_status'] = df['mental_health_status'].fillna('Missing/Inapplicable')\n",
    "\n",
    "# 1.5 Clean up auxiliary columns\n",
    "df = df.drop(columns=['primary_mh_indicator', 'secondary_mh_indicator', 'any_mh_problem'])\n",
    "print(\"\\n--- Feature Engineering 1: Mental Health Status Completed ---\")\n",
    "print(df['mental_health_status'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Feature Engineering: Early Career Unemployment Risk - USER CODE\n",
    "# ==============================================================================\n",
    "\n",
    "# 2.1 Clean and Prepare Variables\n",
    "time_vars = ['n_j1mnth', 'n_j1year', 'n_j1endmnth', 'n_j1endyear', 'n_j1none', 'n_j1still', 'n_jbstat']\n",
    "df = clean_and_convert_to_nullable_int(df, time_vars)\n",
    "\n",
    "# 2.2 Define Threshold for \"Quick Exit\"\n",
    "QUICK_EXIT_THRESHOLD_MONTHS = 12\n",
    "\n",
    "# 2.3 Calculate First Job Duration (in months)\n",
    "df['first_job_duration_months'] = np.where(\n",
    "    df['n_j1year'].notna() & df['n_j1endyear'].notna() & df['n_j1mnth'].notna() & df['n_j1endmnth'].notna(),\n",
    "    (df['n_j1endyear'] - df['n_j1year']) * 12 + (df['n_j1endmnth'] - df['n_j1mnth']),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# 2.4 Construct the Early Career Risk Variable\n",
    "df['early_career_unemp_risk'] = np.nan \n",
    "\n",
    "# Code 2: Never had a paid job\n",
    "df.loc[df['n_j1none'] == 2, 'early_career_unemp_risk'] = 'Never Employed'\n",
    "\n",
    "# Code 1: Still in first job\n",
    "df.loc[df['n_j1still'] == 1, 'early_career_unemp_risk'] = 'Stable Start: Still in First Job'\n",
    "\n",
    "# A. Early Failure: Duration < Threshold\n",
    "df.loc[(df['n_j1still'] == 2) & (df['first_job_duration_months'] < QUICK_EXIT_THRESHOLD_MONTHS),\n",
    "       'early_career_unemp_risk'] = 'Early Failure: Quick Exit'\n",
    "\n",
    "# B. Stable Start: Left Long-Term Job: Duration >= Threshold\n",
    "df.loc[(df['n_j1still'] == 2) & (df['first_job_duration_months'] >= QUICK_EXIT_THRESHOLD_MONTHS),\n",
    "       'early_career_unemp_risk'] = 'Stable Start: Left Long-Term Job'\n",
    "\n",
    "# C. Extended Unemployment after Early Failure (Unemployed now, n_jbstat=3)\n",
    "df.loc[(df['early_career_unemp_risk'] == 'Early Failure: Quick Exit') & (df['n_jbstat'] == 3),\n",
    "       'early_career_unemp_risk'] = 'Extended Unemp after Quick Exit'\n",
    "\n",
    "# 2.5 Finalize Missing / Inapplicable\n",
    "df['early_career_unemp_risk'] = df['early_career_unemp_risk'].fillna('Missing/Inapplicable')\n",
    "       \n",
    "print(\"\\n--- Feature Engineering 2: Early Career Unemployment Risk Completed ---\")\n",
    "print(df['early_career_unemp_risk'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Feature Engineering: Employment Volatility (Employment Stability) - USER CODE\n",
    "# ==============================================================================\n",
    "\n",
    "# 3.1 Clean and Prepare Variables\n",
    "volatility_vars = ['n_nnmpsp_dv', 'n_nmpsp_dv', 'n_nunmpsp_dv']\n",
    "df = clean_and_convert_to_nullable_int(df, volatility_vars)\n",
    "\n",
    "# 3.2 Construct Total Spells (Job changes/state changes)\n",
    "df['total_spells_since_last_int'] = df['n_nnmpsp_dv'] + df['n_nmpsp_dv']\n",
    "\n",
    "# 3.3 Construct Employment Stability Index\n",
    "df['employment_stability_level'] = np.nan\n",
    "\n",
    "# Case 1: Highest Stability - Zero Spells\n",
    "df.loc[df['total_spells_since_last_int'] == 0, \n",
    "       'employment_stability_level'] = 'High Stability: No change reported'\n",
    "       \n",
    "# Case 2: Moderate Stability - One Employment Spell, Zero Non-employment Spells\n",
    "df.loc[(df['n_nmpsp_dv'] == 1) & (df['n_nnmpsp_dv'] == 0), \n",
    "       'employment_stability_level'] = 'Moderate Stability: Single Employment Spell'\n",
    "       \n",
    "# Case 3: Low Stability - Single Non-Employment Spell \n",
    "df.loc[(df['n_nmpsp_dv'] == 0) & (df['n_nnmpsp_dv'] >= 1), \n",
    "       'employment_stability_level'] = 'Low Stability: Non-Employment Spells Only'\n",
    "       \n",
    "# Case 4: High Volatility - Multiple Spells (Threshold >= 3)\n",
    "df.loc[df['total_spells_since_last_int'] >= 3, \n",
    "       'employment_stability_level'] = 'High Volatility: Frequent Changes'\n",
    "\n",
    "# 3.4 Finalize Missing/Inapplicable\n",
    "df['employment_stability_level'] = df['employment_stability_level'].fillna('Missing/Inapplicable')\n",
    "\n",
    "df['unemployment_spells_count'] = df['n_nunmpsp_dv']\n",
    "df = df.drop(columns=['total_spells_since_last_int'])\n",
    "\n",
    "print(\"\\n--- Feature Engineering 3: Employment Stability Completed ---\")\n",
    "print(df['employment_stability_level'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# A. GHQ-12 Score (Using Aggregated Variable n_scghq2_dv)\n",
    "# ==============================================================================\n",
    "ghq_caseness_var = 'n_scghq2_dv'\n",
    "\n",
    "# Clean and rename n_scghq2_dv (already cleaned to float by the initial step)\n",
    "df['ghq12_continuous_score'] = df[ghq_caseness_var].astype(float) \n",
    "\n",
    "print(\"\\n--- Feature Engineering A: GHQ-12 Score (Aggregated) Completed ---\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# B. Education Feature: Ordered Categorical Variable (ISCED-11 based) - REVISED\n",
    "# Goal: Merge Low and Medium Education into 'Non_High_Education' for robustness.\n",
    "# ==============================================================================\n",
    "\n",
    "isced_vars = ['n_isced11_dv', 'n_hiqual_dv']\n",
    "# Assuming clean_and_convert_to_nullable_int is defined and working\n",
    "# df = clean_and_convert_to_nullable_int(df, isced_vars) \n",
    "\n",
    "isced_var = 'n_isced11_dv'\n",
    "education_mapping_revised = {\n",
    "    # Non-High Education: ISCED 0-5 (Codes 9, 2, 3, 4, 5)\n",
    "    9: 'Non_High_Education',    \n",
    "    2: 'Non_High_Education',    \n",
    "    3: 'Non_High_Education',    \n",
    "    4: 'Non_High_Education', \n",
    "    5: 'Non_High_Education', \n",
    "    \n",
    "    # High Education: ISCED 6-8 (Codes 6, 7, 8)\n",
    "    6: 'High_Education',    \n",
    "    7: 'High_Education',    \n",
    "    8: 'High_Education',    \n",
    "}\n",
    "\n",
    "# Apply the revised mapping\n",
    "df['education_level_revised'] = df[isced_var].map(education_mapping_revised).fillna('Missing/Inapplicable')\n",
    "\n",
    "# Define the new ordered categories\n",
    "education_categories_revised = ['Non_High_Education', 'High_Education', 'Missing/Inapplicable']\n",
    "df['education_level_revised'] = pd.Categorical(df['education_level_revised'], \n",
    "                                             categories=education_categories_revised, \n",
    "                                             ordered=True)\n",
    "\n",
    "# OPTIONAL: Replace the original column name if you want to use 'education_level' everywhere\n",
    "# Note: You should update your final column list to use 'education_level_revised' if you keep both.\n",
    "df['education_level'] = df['education_level_revised']\n",
    "df = df.drop(columns=['education_level_revised'])\n",
    "\n",
    "\n",
    "print(\"--- Feature Engineering B: Education Level (Ordered) REVISED: Non-High vs. High Created ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# B.1. Feature Engineering: Vocational Qualification Indicator\n",
    "# Goal: Create a binary indicator if the individual holds any BTEC qualification.\n",
    "# ==============================================================================\n",
    "\n",
    "vocational_vars = ['n_btec1', 'n_btec2', 'n_btec3', 'n_btec4']\n",
    "\n",
    "# 1. Clean and ensure float type for calculation (already handled by initial cleaning, but re-assert)\n",
    "df = clean_missing_codes(df, vocational_vars)\n",
    "\n",
    "# 2. Identify Missingness: Check if all BTEC variables are NaN (Inapplicable/Missing)\n",
    "df['all_btec_nan'] = df[vocational_vars].isnull().all(axis=1)\n",
    "\n",
    "# 3. Create the binary indicator 'has_vocational_qual'\n",
    "# Calculate the maximum value across the 4 BTEC columns. If any is 1.0 (Mentioned), the max is 1.0.\n",
    "df['has_vocational_qual'] = df[vocational_vars].max(axis=1, skipna=True)\n",
    "\n",
    "# 4. Handle Missingness: If all BTEC variables were missing, set the new indicator to NaN.\n",
    "df.loc[df['all_btec_nan'], 'has_vocational_qual'] = np.nan\n",
    "\n",
    "# 5. Convert to nullable integer\n",
    "df = clean_and_convert_to_nullable_int(df, ['has_vocational_qual'])\n",
    "\n",
    "# 6. Clean up auxiliary column\n",
    "df = df.drop(columns=['all_btec_nan'])\n",
    "\n",
    "print(\"\\n--- Feature Engineering B.1: Vocational Qualification Indicator Completed ---\")\n",
    "print(df['has_vocational_qual'].value_counts(dropna=False))\n",
    "\n",
    "# ==============================================================================\n",
    "# C. Control Variables: Binary/Ordinal/Dummy Encoding - CORRECTED\n",
    "# ==============================================================================\n",
    "\n",
    "# Note: We assume core_categorical_vars cleaning (including n_sex, n_health, n_finnow) \n",
    "# to Int64 has already been executed successfully.\n",
    "\n",
    "# C.1. Binary Variables (from codes)\n",
    "\n",
    "# --- CORRECTED CODE FOR 'female' ---\n",
    "# 1. Initialize the column with 0 (Male)\n",
    "df['female'] = 0 \n",
    "# 2. Use .loc to set the '2' (Female) code to 1\n",
    "df.loc[df['n_sex'] == 2, 'female'] = 1\n",
    "# 3. Use .loc to set rows where 'n_sex' is missing (NA) to NaN\n",
    "df.loc[df['n_sex'].isna(), 'female'] = np.nan\n",
    "df['female'] = df['female'].astype('Int64') # Convert final result to Nullable Int\n",
    "\n",
    "# --- CORRECTED CODE FOR 'has_disability' ---\n",
    "# n_health: 1=Yes, 2=No. Recode to 'has_disability' (1=Yes, 0=No).\n",
    "# 1. Initialize the column with 0 (No)\n",
    "df['has_disability'] = 0\n",
    "# 2. Use .loc to set the '1' (Yes) code to 1\n",
    "df.loc[df['n_health'] == 1, 'has_disability'] = 1\n",
    "# 3. Use .loc to set rows where 'n_health' is missing (NA) to NaN\n",
    "df.loc[df['n_health'].isna(), 'has_disability'] = np.nan\n",
    "df['has_disability'] = df['has_disability'].astype('Int64')\n",
    "\n",
    "# --- CORRECTED CODE FOR 'financial_difficulty' ---\n",
    "# n_finnow: 4.0=Quite difficult, 5.0=Very difficult. Recode to 1=Difficulty, 0=No Difficulty.\n",
    "# 1. Initialize the column with 0 (No Difficulty)\n",
    "df['financial_difficulty'] = 0\n",
    "# 2. Use .loc to set the difficulty codes (4, 5) to 1\n",
    "df.loc[df['n_finnow'].isin([4, 5]), 'financial_difficulty'] = 1\n",
    "# 3. Use .loc to set rows where 'n_finnow' is missing (NA) to NaN\n",
    "df.loc[df['n_finnow'].isna(), 'financial_difficulty'] = np.nan\n",
    "df['financial_difficulty'] = df['financial_difficulty'].astype('Int64')\n",
    "\n",
    "\n",
    "# C.2. Categorical Variables: One-Hot Encoding (OHE)\n",
    "\n",
    "# Marital Status (n_marstat)\n",
    "df['n_marstat'] = df['n_marstat'].astype('category')\n",
    "marstat_dummies = pd.get_dummies(df['n_marstat'], prefix='marstat', dummy_na=True, drop_first=False)\n",
    "df = pd.concat([df, marstat_dummies], axis=1)\n",
    "\n",
    "# Ethnicity (n_racel_dv)\n",
    "df['n_racel_dv'] = df['n_racel_dv'].astype('category')\n",
    "racel_dummies = pd.get_dummies(df['n_racel_dv'], prefix='race', dummy_na=True, drop_first=False)\n",
    "df = pd.concat([df, racel_dummies], axis=1)\n",
    "\n",
    "print(\"--- Feature Engineering C: Control Variables (Binary/OHE) Completed ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# D. Advanced Employment Features: Status and Duration - (FIXED: Replacing np.where)\n",
    "# ==============================================================================\n",
    "\n",
    "# D.1. Current Unemployment Status (From n_jbstat)\n",
    "\n",
    "# 1. Binary Indicator: Currently Unemployed\n",
    "df['is_currently_unemployed'] = 0\n",
    "# Use .loc for assignment based on Int64 column\n",
    "df.loc[df['n_jbstat'] == 3, 'is_currently_unemployed'] = 1\n",
    "# Preserve missingness\n",
    "df.loc[df['n_jbstat'].isna(), 'is_currently_unemployed'] = np.nan\n",
    "df['is_currently_unemployed'] = df['is_currently_unemployed'].astype('Int64')\n",
    "\n",
    "\n",
    "# D.2. Key Non-Working/Inactive Indicators (Control Variables)\n",
    "\n",
    "# D.2.1. Long-Term Sick/Disabled (Value 8.0)\n",
    "df['inactive_lt_sick'] = 0\n",
    "df.loc[df['n_jbstat'] == 8, 'inactive_lt_sick'] = 1\n",
    "df.loc[df['n_jbstat'].isna(), 'inactive_lt_sick'] = np.nan\n",
    "df['inactive_lt_sick'] = df['inactive_lt_sick'].astype('Int64')\n",
    "\n",
    "# D.2.2. Looking After Family/Home (Value 6.0)\n",
    "df['inactive_home_family'] = 0\n",
    "df.loc[df['n_jbstat'] == 6, 'inactive_home_family'] = 1\n",
    "df.loc[df['n_jbstat'].isna(), 'inactive_home_family'] = np.nan\n",
    "df['inactive_home_family'] = df['inactive_home_family'].astype('Int64')\n",
    "\n",
    "\n",
    "# D.3. Unemployment Duration (Months)\n",
    "# Note: Duration calculation used only arithmetic, which is safe from this error.\n",
    "\n",
    "date_vars = ['n_jlendm', 'n_jlendy', 'n_ienddatm', 'n_ienddaty']\n",
    "# We assume date_vars were cleaned to Int64 earlier\n",
    "# No change needed for this block, as it handles NA correctly with .notna() and np.where(..., np.nan)\n",
    "# We re-run it here for completeness:\n",
    "df['last_unemployment_duration_months'] = np.where(\n",
    "    df['n_jlendy'].notna() & df['n_ienddaty'].notna() & df['n_jlendm'].notna() & df['n_ienddatm'].notna(),\n",
    "    (df['n_ienddaty'] - df['n_jlendy']) * 12 + (df['n_ienddatm'] - df['n_jlendm']),\n",
    "    np.nan\n",
    ")\n",
    "df.loc[df['n_jbstat'].isin([1, 2]), 'last_unemployment_duration_months'] = np.nan\n",
    "\n",
    "\n",
    "# D.4. Job Search Intensity and Motivation (Auxiliary Indicators)\n",
    "\n",
    "aux_employment_vars = ['n_jbhas', 'n_julk4wk', 'n_julkjb', 'n_jubgn', 'n_jbhad']\n",
    "# We assume aux_employment_vars were cleaned to Int64 earlier\n",
    "\n",
    "# D.4.1. Actively Seeking (Core ILO definition for the unemployed)\n",
    "# n_julk4wk=1 (Yes, looked for work)\n",
    "df['actively_seeking'] = 0\n",
    "df.loc[df['n_julk4wk'] == 1, 'actively_seeking'] = 1\n",
    "df.loc[df['n_julk4wk'].isna(), 'actively_seeking'] = np.nan\n",
    "df['actively_seeking'] = df['actively_seeking'].astype('Int64')\n",
    "\n",
    "# D.4.2. Marginal Attachment/Discouraged Worker \n",
    "# Condition: Did no paid work last week (n_jbhas=2) AND would like a job (n_julkjb=1)\n",
    "df['marginally_attached'] = 0\n",
    "df.loc[(df['n_jbhas'] == 2) & (df['n_julkjb'] == 1), 'marginally_attached'] = 1\n",
    "# Identify missingness where either component is NA\n",
    "missing_mask = df['n_jbhas'].isna() | df['n_julkjb'].isna()\n",
    "df.loc[missing_mask, 'marginally_attached'] = np.nan\n",
    "df['marginally_attached'] = df['marginally_attached'].astype('Int64')\n",
    "\n",
    "\n",
    "# D.4.3. Subjective Employment Chance (n_eprosh)\n",
    "# Recode subjective chance to be predictive (1=likely, 0=unlikely)\n",
    "# 1/2 = Likely, 3/4 = Unlikely\n",
    "df['subjective_job_chance_likely'] = 0\n",
    "df.loc[df['n_eprosh'].isin([1, 2]), 'subjective_job_chance_likely'] = 1\n",
    "df.loc[df['n_eprosh'].isna(), 'subjective_job_chance_likely'] = np.nan\n",
    "df['subjective_job_chance_likely'] = df['subjective_job_chance_likely'].astype('Int64')\n",
    "\n",
    "print(\"\\n--- Feature Engineering D: Advanced Employment Features (Fixed) Completed ---\")\n",
    "print(df[['is_currently_unemployed', 'last_unemployment_duration_months', 'marginally_attached']].describe())\n",
    "\n",
    "# ==============================================================================\n",
    "# E. Demographic and Socioeconomic Control Variables (FIXED)\n",
    "# Goal: Finalize encoding for age, marital status (using n_marstat), ethnicity, and financial perception.\n",
    "# ==============================================================================\n",
    "\n",
    "# Variables to ensure initial cleaning/Int64 conversion:\n",
    "control_vars_to_clean = ['n_age_dv', 'n_health', 'n_marstat', 'n_racel_dv', \n",
    "                         'n_urban_dv', 'n_finnow', 'n_finfut']\n",
    "df = clean_and_convert_to_nullable_int(df, control_vars_to_clean) # Assuming this function is correctly defined\n",
    "\n",
    "# E.1. Continuous Variable: Age (n_age_dv)\n",
    "# Age is already cleaned as Int64.\n",
    "\n",
    "# E.2. Binary Variables (n_urban_dv)\n",
    "# n_urban_dv: 1=Urban, 2=Rural. Create 'is_urban' (1=Urban, 0=Rural).\n",
    "df['is_urban'] = 0\n",
    "df.loc[df['n_urban_dv'] == 1, 'is_urban'] = 1\n",
    "df.loc[df['n_urban_dv'].isna(), 'is_urban'] = np.nan\n",
    "df['is_urban'] = df['is_urban'].astype('Int64')\n",
    "\n",
    "# E.3. Ordinal/Binary Financial Variables \n",
    "# n_finnow -> financial_difficulty (already handled in previous steps using .loc)\n",
    "\n",
    "# n_finfut: Subjective financial situation - future (1=Better, 2=Worse, 3=Same)\n",
    "df['n_finfut'] = df['n_finfut'].astype('category')\n",
    "finfut_dummies = pd.get_dummies(df['n_finfut'], prefix='finfut', dummy_na=True, drop_first=False)\n",
    "df = pd.concat([df, finfut_dummies], axis=1)\n",
    "\n",
    "\n",
    "# E.4. High-Dimensional Categorical Variable: Ethnicity (n_racel_dv)\n",
    "# Goal: Group 17 detailed codes into broader, more statistically meaningful categories (White, Asian, Black, etc.).\n",
    "\n",
    "# Define the grouping map based on standard UK research practice:\n",
    "ethnic_group_map = {\n",
    "    1: 'White', 2: 'White', 3: 'White', 4: 'White',\n",
    "    5: 'Mixed', 6: 'Mixed', 7: 'Mixed', 8: 'Mixed',\n",
    "    9: 'Asian', 10: 'Asian', 11: 'Asian', 12: 'Asian', 13: 'Asian',\n",
    "    14: 'Black', 15: 'Black', 16: 'Black',\n",
    "    17: 'Other', 97: 'Other'\n",
    "}\n",
    "\n",
    "df['ethnic_group'] = df['n_racel_dv'].map(ethnic_group_map)\n",
    "\n",
    "# Apply One-Hot Encoding to the grouped variable\n",
    "df['ethnic_group'] = df['ethnic_group'].astype('category')\n",
    "ethnic_dummies = pd.get_dummies(df['ethnic_group'], prefix='race_grp', dummy_na=True, drop_first=False)\n",
    "df = pd.concat([df, ethnic_dummies], axis=1)\n",
    "\n",
    "\n",
    "# E.5. Categorical Variable: Marital Status (n_marstat) - FIXED\n",
    "marital_var_name = 'n_marstat' # Use the correct column name\n",
    "\n",
    "# 1. Handle the 'Under 16 years' code (0.0) if present, converting it to NaN/NA.\n",
    "# This ensures that non-adults are excluded from the adult marital status analysis.\n",
    "df[marital_var_name] = df[marital_var_name].replace({0.0: np.nan})\n",
    "\n",
    "# 2. Apply One-Hot Encoding\n",
    "df[marital_var_name] = df[marital_var_name].astype('category')\n",
    "marstat_dummies_final = pd.get_dummies(df[marital_var_name], prefix='marstat_final', dummy_na=True, drop_first=False)\n",
    "df = pd.concat([df, marstat_dummies_final], axis=1)\n",
    "\n",
    "\n",
    "print(\"\\n--- Feature Engineering E: Demographic and Control Variables (Fixed) Completed ---\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Feature Engineering E: Demographic and Control Variables Completed ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# F. Feature Engineering: Previous Wave Mental Health Indicators\n",
    "# ==============================================================================\n",
    "\n",
    "# F.1. Clean and Prepare Variables (Including previous health and life satisfaction)\n",
    "prev_mh_vars = ['scghq2_dv_prev', 'sclfsato_prev', 'health_prev', 'sf12mcs_dv_prev']\n",
    "df = clean_and_convert_to_nullable_float(df, prev_mh_vars)\n",
    "\n",
    "# F.2. Introduce Previous GHQ-12 Score (0-12 Caseness)\n",
    "df['prev_ghq12_score'] = df['scghq2_dv_prev'].astype(float)\n",
    "\n",
    "# F.3. Calculate GHQ Score Change (Current - Previous)\n",
    "# Positive value means worsening mental health, negative means improvement.\n",
    "df['ghq12_score_change'] = df['ghq12_continuous_score'] - df['prev_ghq12_score']\n",
    "\n",
    "print(\"\\n--- Feature Engineering F: Previous Wave MH Indicators Completed ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# G. Feature Engineering: Loneliness/Social Isolation Index\n",
    "# ==============================================================================\n",
    "\n",
    "lonely_vars = ['n_scisolate', 'n_sclackcom', 'n_scleftout', 'n_sclonely']\n",
    "\n",
    "# G.1. Clean and Prepare Variables (Assuming they are Likert scale 1-4)\n",
    "df = clean_and_convert_to_nullable_float(df, lonely_vars)\n",
    "\n",
    "# G.2. Construct Loneliness Score (Higher value = Higher Loneliness)\n",
    "# Assuming 1=Most Frequent/Highest Degree, 4=Least Frequent/Lowest Degree.\n",
    "# Recode: 1->4, 2->3, 3->2, 4->1. Use 5 - value.\n",
    "df['loneliness_score'] = df[lonely_vars].apply(\n",
    "    lambda row: sum(5 - x for x in row if pd.notna(x)),\n",
    "    axis=1\n",
    ")\n",
    "# Ensure that if all components are NaN, the final score is NaN.\n",
    "df['all_lonely_nan'] = df[lonely_vars].isnull().all(axis=1)\n",
    "df.loc[df['all_lonely_nan'], 'loneliness_score'] = np.nan\n",
    "df = df.drop(columns=['all_lonely_nan'])\n",
    "\n",
    "print(\"\\n--- Feature Engineering G: Loneliness/Social Isolation Index Completed ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# H. Feature Engineering: Health Behavior (Smoking Status)\n",
    "# ==============================================================================\n",
    "\n",
    "smoking_vars = ['n_smoker', 'n_ncigs']\n",
    "df = clean_and_convert_to_nullable_int(df, smoking_vars)\n",
    "\n",
    "# H.1. Binary Indicator: Current Smoker\n",
    "# Recode: 1 -> 1 (Smoker), 2/3 -> 0 (Non-Smoker). Preserve NaN.\n",
    "df['is_smoker'] = 0\n",
    "df.loc[df['n_smoker'] == 1, 'is_smoker'] = 1\n",
    "# Set rows where n_smoker is NaN back to NaN\n",
    "df.loc[df['n_smoker'].isna(), 'is_smoker'] = np.nan\n",
    "df['is_smoker'] = df['is_smoker'].astype('Int64')\n",
    "\n",
    "\n",
    "# H.2. Binary Indicator: Heavy Smoker (e.g., >= 20 cigarettes/day)\n",
    "# Note: n_ncigs is NaN for non-smokers/missing.\n",
    "df['heavy_smoker'] = 0\n",
    "df.loc[df['n_ncigs'] >= 20, 'heavy_smoker'] = 1\n",
    "# Preserve missingness from n_ncigs\n",
    "df.loc[df['n_ncigs'].isna(), 'heavy_smoker'] = np.nan\n",
    "df['heavy_smoker'] = df['heavy_smoker'].astype('Int64')\n",
    "\n",
    "print(\"\\n--- Feature Engineering H: Health Behavior (Smoking Status) Completed ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# I. Feature Engineering: Objective Income and Economic Pressure\n",
    "# ==============================================================================\n",
    "\n",
    "income_vars = ['n_fimnnet_dv', 'n_fimnsben_dv']\n",
    "df = clean_and_convert_to_nullable_float(df, income_vars)\n",
    "\n",
    "# I.1. Total Net Income (Log transformed)\n",
    "# Use log1p(x) for smooth transformation, handling 0 or small values safely.\n",
    "df['log_total_income'] = df['n_fimnnet_dv'].apply(lambda x: np.log1p(x) if pd.notna(x) and x >= 0 else np.nan)\n",
    "\n",
    "\n",
    "# I.2. Benefit Income Ratio (Dependency on Social Welfare)\n",
    "df['benefit_income_ratio'] = df['n_fimnsben_dv'] / df['n_fimnnet_dv']\n",
    "\n",
    "# If total income is non-positive, set to NaN to avoid Inf/errors/invalid ratio\n",
    "df.loc[df['n_fimnnet_dv'] <= 0, 'benefit_income_ratio'] = np.nan\n",
    "# If benefit income is 0 and total income > 0, the result will be 0 (correct).\n",
    "\n",
    "print(\"\\n--- Feature Engineering I: Objective Income and Economic Pressure Completed ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# J. Feature Engineering: Socioeconomic Class (NS-SEC)\n",
    "# ==============================================================================\n",
    "\n",
    "nssec_var = 'n_jbnssec8_dv'\n",
    "df = clean_and_convert_to_nullable_int(df, [nssec_var])\n",
    "\n",
    "# J.1. Categorical Variable: NS-SEC 8-class (Grouped/Mapped)\n",
    "nssec_map = {\n",
    "    1: 'Higher_managerial',   # Higher managerial/professional occupations\n",
    "    2: 'Lower_managerial',    # Lower managerial/professional occupations\n",
    "    3: 'Intermediate',        # Intermediate occupations\n",
    "    4: 'Small_employers',     # Small employers and own account workers\n",
    "    5: 'Lower_supervisory',   # Lower supervisory and technical occupations\n",
    "    6: 'Semi_routine',        # Semi-routine occupations\n",
    "    7: 'Routine',             # Routine occupations\n",
    "    8: 'Never_worked',        # Never worked and long-term unemployed\n",
    "}\n",
    "df['nssec_class'] = df[nssec_var].map(nssec_map).fillna('Missing/Inapplicable')\n",
    "df['nssec_class'] = df['nssec_class'].astype('category')\n",
    "\n",
    "# J.2. Binary Indicator: High Status Job (NS-SEC 1 or 2)\n",
    "df['high_status_job'] = 0\n",
    "df.loc[df[nssec_var].isin([1, 2]), 'high_status_job'] = 1\n",
    "df.loc[df[nssec_var].isna(), 'high_status_job'] = np.nan\n",
    "df['high_status_job'] = df['high_status_job'].astype('Int64')\n",
    "\n",
    "# J.3. One-Hot Encode the Categorical NS-SEC\n",
    "nssec_dummies = pd.get_dummies(df['nssec_class'], prefix='nssec', drop_first=False)\n",
    "df = pd.concat([df, nssec_dummies], axis=1)\n",
    "df = df.drop(columns=['nssec_class']) # Drop the intermediate categorical column\n",
    "\n",
    "print(\"\\n--- Feature Engineering J: Socioeconomic Class (NS-SEC) Completed ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# K. Feature Engineering: Job Exit Reason Indicators\n",
    "# ==============================================================================\n",
    "\n",
    "job_end_var = 'n_jbterm1'\n",
    "df = clean_and_convert_to_nullable_int(df, [job_end_var])\n",
    "\n",
    "# K.1. Binary Indicator: Job End Due to Health/Stress/Family (Placeholder Codes based on UKHLS-like structure)\n",
    "# Assuming: 4=Illness/Disability, 5=Pregnancy, 6=Stress/Mental Health\n",
    "health_exit_codes = [4, 5, 6] \n",
    "df['job_end_due_to_health'] = 0\n",
    "df.loc[df[job_end_var].isin(health_exit_codes), 'job_end_due_to_health'] = 1\n",
    "df.loc[df[job_end_var].isna(), 'job_end_due_to_health'] = np.nan\n",
    "df['job_end_due_to_health'] = df['job_end_due_to_health'].astype('Int64')\n",
    "\n",
    "# K.2. Binary Indicator: Involuntary Job End (Placeholder Codes based on UKHLS-like structure)\n",
    "# Assuming: 1=Dismissed, 2=Made Redundant/Laid Off\n",
    "involuntary_codes = [1, 2]\n",
    "df['job_end_involuntary'] = 0\n",
    "df.loc[df[job_end_var].isin(involuntary_codes), 'job_end_involuntary'] = 1\n",
    "df.loc[df[job_end_var].isna(), 'job_end_involuntary'] = np.nan\n",
    "df['job_end_involuntary'] = df['job_end_involuntary'].astype('Int64')\n",
    "\n",
    "print(\"\\n--- Feature Engineering K: Job Exit Reason Indicators Completed ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# L. Feature Engineering: Geographic Region Dummies\n",
    "# ==============================================================================\n",
    "\n",
    "region_var = 'n_gor_dv'\n",
    "df = clean_and_convert_to_nullable_int(df, [region_var])\n",
    "\n",
    "# L.1. Map GOR codes to Region Names (UK Government Office Regions)\n",
    "region_map = {1:'NorthEast', 2:'NorthWest', 3:'YorkshireHumber', 4:'EastMidlands', \n",
    "              5:'WestMidlands', 6:'EastEngland', 7:'London', 8:'SouthEast',\n",
    "              9:'SouthWest', 10:'Wales', 11:'Scotland', 12:'N_Ireland'}\n",
    "df['region_name'] = df[region_var].map(region_map).fillna('Missing/Inapplicable')\n",
    "\n",
    "# L.2. One-Hot Encode Region\n",
    "df['region_name'] = df['region_name'].astype('category')\n",
    "# We use dummy_na=False here because 'Missing/Inapplicable' is an explicit category\n",
    "region_dummies = pd.get_dummies(df['region_name'], prefix='region', dummy_na=False, drop_first=False) \n",
    "df = pd.concat([df, region_dummies], axis=1)\n",
    "df = df.drop(columns=['region_name'])\n",
    "\n",
    "print(\"\\n--- Feature Engineering L: Geographic Region Dummies Completed ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Final Data Prep and Save (Complete and Optimized) - UPDATED\n",
    "# Goal: Keep only final engineered features and essential IDs/controls.\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. List of ALL desired final columns\n",
    "columns_to_keep = [\n",
    "    # 1. IDs/Raw Continuous/Counters \n",
    "    'pidp', 'n_age_dv', 'n_hhsize', 'n_nchild_dv', 'unemployment_spells_count', \n",
    "    'n_ncigs', # Raw smoking count (H)\n",
    "    \n",
    "    # 2. Outcomes & Previous Wave MH (F)\n",
    "    'mental_health_status', 'ghq12_continuous_score',\n",
    "    'prev_ghq12_score', 'ghq12_score_change', \n",
    "    \n",
    "    # 3. Core Predictors (Categorical/Ordinal)\n",
    "    'early_career_unemp_risk', 'employment_stability_level', 'education_level',\n",
    "    \n",
    "    # 4. Employment Features (Binary/Continuous) - Step D, J, K\n",
    "    'is_currently_unemployed', 'last_unemployment_duration_months', \n",
    "    'inactive_lt_sick', 'inactive_home_family',\n",
    "    'actively_seeking', 'marginally_attached', 'subjective_job_chance_likely',\n",
    "    'high_status_job', 'job_end_due_to_health', 'job_end_involuntary',\n",
    "    \n",
    "    # 5. Controls (Continuous/Ordinal/Simple Binary) - Steps C, E, F, G, H, I\n",
    "    'female', 'has_disability', 'financial_difficulty', 'is_urban',\n",
    "    'has_vocational_qual', 'health_prev', 'sclfsato_prev', 'sf12mcs_dv_prev', \n",
    "    'loneliness_score', 'is_smoker', 'heavy_smoker',\n",
    "    'log_total_income', 'benefit_income_ratio',\n",
    "\n",
    "]\n",
    "\n",
    "# 6. Add all generated dummy columns (One-Hot Encoded variables)\n",
    "# Collect all prefixes used for OHE variables across all steps\n",
    "dummy_prefixes = (\\\n",
    "    'marstat_final_', # Final Marital Status (n_marstat_dv fix)\n",
    "    'race_grp_',      # Ethnic Grouping (Step E)\n",
    "    'finfut_',        # Future Financial Status (Step E)\n",
    "    'nssec_',         # NS-SEC Social Class (Step J)\n",
    "    'region_',        # Geographic Region (Step L)\n",
    "    'marstat_',       # Old Marital Status (if any)\n",
    "    'race_',          # Old Ethnicity (if any)\n",
    ")\n",
    "\n",
    "# Dynamically capture all columns starting with the specified prefixes\n",
    "dummy_cols = [col for col in df.columns if col.startswith(dummy_prefixes)]\n",
    "\n",
    "# Combine and ensure final columns only include those existing in the DataFrame\n",
    "final_columns = list(set(columns_to_keep + dummy_cols))\n",
    "final_columns = [col for col in final_columns if col in df.columns]\n",
    "\n",
    "# Filter the DataFrame to keep only the final model columns\n",
    "df_final = df[final_columns].copy()\n",
    "\n",
    "# Save the optimized file\n",
    "output_file_name = \"n_indresp_FINAL_FEATURES_OPTIMIZED02.csv\" # Updated name to reflect new features\n",
    "output_file_path = os.path.join(processed_dir, output_file_name)\n",
    "df_final.to_csv(output_file_path, index=False)\n",
    "print(f\"\\n--- ALL Feature Engineering Stages Complete. Optimized data saved to: {output_file_path} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769de0de-57c6-4817-abfa-f01f585c2d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
