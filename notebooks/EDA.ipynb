{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba28361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ËÆæÁΩÆÈ°πÁõÆÊ†πÁõÆÂΩïË∑ØÂæÑÔºå‰øÆÂ§çÊ®°ÂùóÂØºÂÖ•ÈóÆÈ¢ò ---\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# --- ÂØºÂÖ•ÈÖçÁΩÆÊ®°Âùó ---\n",
    "from utils.config import load_config, abspath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599e4b2",
   "metadata": {},
   "source": [
    "## üîß Âä†ËΩΩÈÖçÁΩÆ‰∏éÂ≠óÊÆµÊ∏ÖÂçïÔºàÁªü‰∏ÄÂ≠óÊÆµÊéßÂà∂ via candidate_vars_csvÔºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91e5c174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂÄôÈÄâÂ≠óÊÆµÊÄªÊï∞Ôºö 118\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme</th>\n",
       "      <th>varname</th>\n",
       "      <th>label_in_dictionary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>demographics</td>\n",
       "      <td>n_age_dv</td>\n",
       "      <td>Age, derived from dob_dv and intdat_dv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demographics</td>\n",
       "      <td>n_agegr5_dv</td>\n",
       "      <td>Age group (age_dv): 5 year intervals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demographics</td>\n",
       "      <td>n_englang</td>\n",
       "      <td>English is first language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>demographics</td>\n",
       "      <td>n_ethn_dv</td>\n",
       "      <td>Ethnic group (derived from multiple sources)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>demographics</td>\n",
       "      <td>n_gor_dv</td>\n",
       "      <td>Government Office Region</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          theme      varname                           label_in_dictionary\n",
       "0  demographics     n_age_dv        Age, derived from dob_dv and intdat_dv\n",
       "1  demographics  n_agegr5_dv          Age group (age_dv): 5 year intervals\n",
       "2  demographics    n_englang                     English is first language\n",
       "3  demographics    n_ethn_dv  Ethnic group (derived from multiple sources)\n",
       "4  demographics     n_gor_dv                      Government Office Region"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.config import load_config, abspath\n",
    "import pandas as pd\n",
    "\n",
    "cfg, root = load_config()\n",
    "\n",
    "# Âä†ËΩΩÂ§ÑÁêÜÂêéÁöÑ parquet Êï∞ÊçÆÔºàÁâπÂæÅÔºâ\n",
    "df = pd.read_parquet(abspath(root, cfg[\"processing\"][\"outputs\"][\"features\"]))\n",
    "df_raw = pd.read_parquet(abspath(root, cfg[\"processing\"][\"outputs\"][\"indresp_selected\"]))\n",
    "\n",
    "# Âä†ËΩΩÂ≠óÊÆµÂÄôÈÄâÂàóË°®\n",
    "candidate_vars_path = abspath(root, cfg[\"ukhls\"][\"candidate_vars_csv\"])\n",
    "df_vars = pd.read_csv(candidate_vars_path)\n",
    "usecols = df_vars[\"varname\"].tolist()\n",
    "\n",
    "print(\"ÂÄôÈÄâÂ≠óÊÆµÊÄªÊï∞Ôºö\", len(usecols))\n",
    "display(df_vars.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21067339-1ed8-4059-80d7-d317ff7768de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# ------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49d11651-a846-4fd8-af9e-395a307e84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display setting\n",
    "# ------------------------------------\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21d31bfc-8967-4cb9-8dd6-0f01d1553c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.Set File Path\n",
    "# ------------------------------------\n",
    "file_path = \"abspath(root, cfg['ukhls']['indresp_dta'])\"\n",
    "output_dir = abspath(root, cfg['paths']['data_processed'])  # updated\"abspath(root, cfg['paths']['data_processed'])\"\n",
    "output_file_name = \"n_indresp_navigation.csv\"\n",
    "output_path = os.path.join(output_dir, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "263f95ee-830f-4f2b-ab50-a578597d2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Variables to Extract (use_vars) - All prefixed with 'n_' for Wave 14\n",
    "# ------------------------------------\n",
    "use_vars = [\n",
    "    # 1. Identifiers\n",
    "    \"pidp\",     # Cross-wave person identifier (Core ID)\n",
    "    \"pid\",      # personal identifier (BHPS cohort)\n",
    "    \"n_hidp\",   # Household identifier \n",
    "    \n",
    "    # 2. Mental Health Outcomes\n",
    "    \"n_scghq1_dv\",  # GHQ-12 Score (Likert, 0-36) - Continuous target\n",
    "    \"n_scghq2_dv\",  # GHQ-12 Caseness (Binary) - Classification target\n",
    "    \"n_sclfsato\",   # Satisfaction with life overall (SWB proxy)\n",
    "    \"n_health\",     # Self-reported general health (Control)\n",
    "\n",
    "    # Specific Diagnosed Conditions\n",
    "    \"n_mhcond8\",    # A phobia\n",
    "    \"n_mhcond9\",    # Panic attacks\n",
    "    \"n_mhcond6\",    # Post-traumatic stress disorder (PTSD)\n",
    "    \"n_mhcond19\",   # Any other mental, emotional or neurological problem or condition\n",
    "    \"n_mhcond10\",   # Attention deficit hyperactivity disorder (ADHD)\n",
    "    \"n_mhcond4\",    # Bipolar disorder (or 'manic depression')\n",
    "    \"n_mhcond2\",    # Depression\n",
    "    \"n_mhcond11\",   # Post-natal depression\n",
    "    \"n_mhcond12\",   # Dementia (including Alzheimer's)\n",
    "    \"n_mhcond5\",    # An eating disorder\n",
    "    \"n_mhcond13\",   # Nervous breakdown\n",
    "    \"n_mhcond14\",   # A personality disorder\n",
    "    \"n_mhcond3\",    # Psychosis or schizophrenia\n",
    "    \"n_mhcond15\",   # Obsessive compulsive disorder (OCD)\n",
    "    \"n_mhcond16\",   # Seasonal affective disorder\n",
    "    \"n_mhcond17\",   # Alcohol or drug dependence\n",
    "    \"n_mhcond18\",   # Any other anxiety disorder\n",
    "    \"n_mhcond97\",   # Any other emotional, nervous or psychiatric problem or condition\n",
    "    \"n_mhcond96\",   # None of these (Useful for validation)\n",
    "    \n",
    "    # 3. Demographics and Control Variables\n",
    "    \"n_sex\",        # Sex\n",
    "    \"n_age_dv\",     # Age\n",
    "    \"n_marstat\",    # Marital Status\n",
    "    \"n_racel_dv\",   # Ethnic Group\n",
    "    \"n_nchild_dv\",  # Number of children in household\n",
    "    \"n_hhsize\",     # Household size\n",
    "    \"n_urban_dv\",   # Urban or rural area\n",
    "    \"n_finnow\",     # Subjective financial situation - now\n",
    "    \"n_finfut\",     # Subjective financial situation - future\n",
    "\n",
    "    # Core Controls and Utility Variables (Finance, Date, Weight)\n",
    "    # Household Net Monthly Income (Financial Control)\n",
    "    \"n_ienddaty\",    # Interview Year (CRITICAL for Duration Calculation)\n",
    "    \"n_ienddatm\",    # Interview Month (CRITICAL for Duration Calculation)\n",
    "    \"n_ienddatd\",\n",
    "    \"n_indinub_lw\", # Individual Longitudinal Weight (For Statistical Inference)\n",
    "\n",
    "    # 4. Employment & Unemployment Features\n",
    "    \"n_jbstat\",     # Main economic activity (Current status)\n",
    "    \"n_jbhas\",      # Had paid work last week\n",
    "    \"n_jbsemp\",     # Self-employed\n",
    "    \"n_julk4wk\",    # Looked for work in the last 4 weeks\n",
    "    \"n_julkjb\",     # Would like a regular paid job\n",
    "    \"n_jbhad\",      # Ever had a paid job? (General history)\n",
    "    \"n_jubgn\",      # Able to start work within 2 weeks\n",
    "    \"n_eprosh\",     # Chance starting work within 12 months\n",
    "    \n",
    "    # Last Job End Date (for Current Unemployment Duration)\n",
    "    \"n_jlendm\",     # Month left last job\n",
    "    \"n_jlendy\",     # Year left last job\n",
    "\n",
    "    # Unemployment History\n",
    "    \"n_nnmpsp_dv\",  # No. non-employment spells since last interview\n",
    "    \"n_nmpsp_dv\",   # No. employment spells since last interview\n",
    "    \"n_nunmpsp_dv\", # No. unemployment spells since last interview   \n",
    "\n",
    "    # First Job Status and Duration (Advanced Feature Construction)\n",
    "    \"n_j1none\",     # Still in full-time education / never had a paid job (Used for \"Never Employed\")\n",
    "    \"n_j1still\",    # Still in first job (Used to flag censored duration data)\n",
    "    \"n_j1mnth\",     # First job start month\n",
    "    \"n_j1year\",     # First job start year\n",
    "    \"n_j1endmnth\",  # First job end month\n",
    "    \"n_j1endyear\",  # First job end year\n",
    "    \n",
    "    # 5. Education Variables\n",
    "    \"n_edtype\",     # Type of educational institution attending \n",
    "    \"n_hiqual_dv\",  # Highest educational qualification (UK standard)\n",
    "    \"n_isced11_dv\", # Highest education qualification, short ISCED 2011\n",
    "    \"n_qfhigh\",     # Highest qualification level\n",
    "    \"n_btec1\",      # First certificate or general certificate (below level 2)\n",
    "    \"n_btec2\",      # First diploma or general diploma (level 2)\n",
    "    \"n_btec3\",      # National Certificate or National Diploma level (level 3)\n",
    "    \"n_btec4\",      # Higher level (level 4 or higher)\n",
    "    \n",
    "    # Proxies for Total Years of Education (for CFPS comparability)\n",
    "    \"n_feend\",      # Age finished further education\n",
    "    \"n_scend\"       # Age finished school\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "060e7507-d004-4388-8e89-c89988b4cd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File not found. Please confirm the file path is correct: abspath(root, cfg['ukhls']['indresp_dta'])\n"
     ]
    }
   ],
   "source": [
    "# 2.Data Reading Code\n",
    "# ------------------------------------\n",
    "try:\n",
    "    # Attempt to read the Stata file using the specified path and variables\n",
    "    df = pd.read_stata(file_path, \n",
    "                       columns=use_vars)\n",
    "    print(f\"Successfully extracted data from {file_path}.\")\n",
    "    print(f\"DataFrame dimensions: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows of the data (Preview):\")\n",
    "    # print(df.head()) # Uncomment to view the head in your environment\n",
    "    \n",
    "    # Next Step: Missing Value Handling and Feature Engineering\n",
    "    import numpy as np\n",
    "    # Replace UKHLS special missing value codes with NaN\n",
    "    # Common codes: -9 (missing), -8 (inapplicable), -7 (proxy), -2 (refusal), -1 (don't know)\n",
    "    df = df.replace([-9, -8, -7, -2, -1], np.nan)\n",
    "    print(\"\\nInitial cleaning: UKHLS missing codes replaced with NaN.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found. Please confirm the file path is correct: {file_path}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: An issue occurred during data reading (e.g., incorrect variable names or Stata file issue). Details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a11594c-7817-483c-a94b-6b482aabddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved the extracteddata to:\n",
      "--> /Users/wanderer/Thesis/ModelProject/Processed_Data/n_indresp_navigation.csv\n"
     ]
    }
   ],
   "source": [
    "# 3.Data Storage (Saving to CSV)\n",
    "# ------------------------------------\n",
    "if df is not None:\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nSuccessfully saved the extracteddata to:\")\n",
    "        print(f\"--> {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file to CSV: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43aa11e0-68bf-466f-acea-55e406092922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# ------------------------------------\n",
    "processed_dir = \"abspath(root, cfg['paths']['data_processed'])\"\n",
    "file_name = \"n_indresp_navigation.csv\"\n",
    "file_path = os.path.join(processed_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0aae4a06-a024-42c3-9e8f-1a38ac9a16e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading and Feature Engineering ---\n",
      "Successfully loaded file: /Users/wanderer/Thesis/ModelProject/Processed_Data/n_indresp_navigation.csv. Dimensions: (35471, 68)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Data Loading and Feature Engineering ---\")\n",
    "\n",
    "try:\n",
    "    # Load the extracted and cleaned CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded file: {file_path}. Dimensions: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found. Please confirm the path is correct: {file_path}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fa56560-bb48-46b0-8e46-a616f7cd4f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pidp</th>\n",
       "      <th>pid</th>\n",
       "      <th>n_hidp</th>\n",
       "      <th>n_scghq1_dv</th>\n",
       "      <th>n_scghq2_dv</th>\n",
       "      <th>n_sclfsato</th>\n",
       "      <th>n_health</th>\n",
       "      <th>n_mhcond8</th>\n",
       "      <th>n_mhcond9</th>\n",
       "      <th>n_mhcond6</th>\n",
       "      <th>n_mhcond19</th>\n",
       "      <th>n_mhcond10</th>\n",
       "      <th>n_mhcond4</th>\n",
       "      <th>n_mhcond2</th>\n",
       "      <th>n_mhcond11</th>\n",
       "      <th>n_mhcond12</th>\n",
       "      <th>n_mhcond5</th>\n",
       "      <th>n_mhcond13</th>\n",
       "      <th>n_mhcond14</th>\n",
       "      <th>n_mhcond3</th>\n",
       "      <th>n_mhcond15</th>\n",
       "      <th>n_mhcond16</th>\n",
       "      <th>n_mhcond17</th>\n",
       "      <th>n_mhcond18</th>\n",
       "      <th>n_mhcond97</th>\n",
       "      <th>n_mhcond96</th>\n",
       "      <th>n_sex</th>\n",
       "      <th>n_age_dv</th>\n",
       "      <th>n_marstat</th>\n",
       "      <th>n_racel_dv</th>\n",
       "      <th>n_nchild_dv</th>\n",
       "      <th>n_hhsize</th>\n",
       "      <th>n_urban_dv</th>\n",
       "      <th>n_finnow</th>\n",
       "      <th>n_finfut</th>\n",
       "      <th>n_ienddaty</th>\n",
       "      <th>n_ienddatm</th>\n",
       "      <th>n_ienddatd</th>\n",
       "      <th>n_indinub_lw</th>\n",
       "      <th>n_jbstat</th>\n",
       "      <th>n_jbhas</th>\n",
       "      <th>n_jbsemp</th>\n",
       "      <th>n_julk4wk</th>\n",
       "      <th>n_julkjb</th>\n",
       "      <th>n_jbhad</th>\n",
       "      <th>n_jubgn</th>\n",
       "      <th>n_eprosh</th>\n",
       "      <th>n_jlendm</th>\n",
       "      <th>n_jlendy</th>\n",
       "      <th>n_nnmpsp_dv</th>\n",
       "      <th>n_nmpsp_dv</th>\n",
       "      <th>n_nunmpsp_dv</th>\n",
       "      <th>n_j1none</th>\n",
       "      <th>n_j1still</th>\n",
       "      <th>n_j1mnth</th>\n",
       "      <th>n_j1year</th>\n",
       "      <th>n_j1endmnth</th>\n",
       "      <th>n_j1endyear</th>\n",
       "      <th>n_edtype</th>\n",
       "      <th>n_hiqual_dv</th>\n",
       "      <th>n_isced11_dv</th>\n",
       "      <th>n_qfhigh</th>\n",
       "      <th>n_btec1</th>\n",
       "      <th>n_btec2</th>\n",
       "      <th>n_btec3</th>\n",
       "      <th>n_btec4</th>\n",
       "      <th>n_feend</th>\n",
       "      <th>n_scend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22445</td>\n",
       "      <td>10127798</td>\n",
       "      <td>276365626</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>Somewhat satisfied</td>\n",
       "      <td>No</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>female</td>\n",
       "      <td>37</td>\n",
       "      <td>Married</td>\n",
       "      <td>british/english/scottish/welsh/northern irish ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>urban area</td>\n",
       "      <td>Doing alright</td>\n",
       "      <td>or about the same?</td>\n",
       "      <td>2022</td>\n",
       "      <td>April</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Paid employment(ft/pt)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Employee</td>\n",
       "      <td>No</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>Degree</td>\n",
       "      <td>Masters or equivalent</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29925</td>\n",
       "      <td>10192697</td>\n",
       "      <td>617732426</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>Somewhat dissatisfied</td>\n",
       "      <td>Yes</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>british/english/scottish/welsh/northern irish ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>urban area</td>\n",
       "      <td>Finding it very difficult</td>\n",
       "      <td>or about the same?</td>\n",
       "      <td>2022</td>\n",
       "      <td>August</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Paid employment(ft/pt)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Employee</td>\n",
       "      <td>No</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>Degree</td>\n",
       "      <td>Masters or equivalent</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76165</td>\n",
       "      <td>10689869</td>\n",
       "      <td>140161626</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>Mostly satisfied</td>\n",
       "      <td>No</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>female</td>\n",
       "      <td>39</td>\n",
       "      <td>Married</td>\n",
       "      <td>british/english/scottish/welsh/northern irish ...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>urban area</td>\n",
       "      <td>Just about getting by</td>\n",
       "      <td>Better off</td>\n",
       "      <td>2022</td>\n",
       "      <td>March</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Paid employment(ft/pt)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Employee</td>\n",
       "      <td>No</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>A-level etc</td>\n",
       "      <td>Upper secondary</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>280165</td>\n",
       "      <td>12430439</td>\n",
       "      <td>753467226</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>Somewhat dissatisfied</td>\n",
       "      <td>No</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>female</td>\n",
       "      <td>43</td>\n",
       "      <td>Married</td>\n",
       "      <td>british/english/scottish/welsh/northern irish ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>rural area</td>\n",
       "      <td>Just about getting by</td>\n",
       "      <td>or about the same?</td>\n",
       "      <td>2022</td>\n",
       "      <td>December</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Self employed</td>\n",
       "      <td>No</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>Yes</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Very likely</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>GCSE etc</td>\n",
       "      <td>Lower secondary</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>469205</td>\n",
       "      <td>13857142</td>\n",
       "      <td>413725626</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>Mostly satisfied</td>\n",
       "      <td>No</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>female</td>\n",
       "      <td>32</td>\n",
       "      <td>Single, nvr marr/civ p</td>\n",
       "      <td>british/english/scottish/welsh/northern irish ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>urban area</td>\n",
       "      <td>Doing alright</td>\n",
       "      <td>or about the same?</td>\n",
       "      <td>2022</td>\n",
       "      <td>May</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Paid employment(ft/pt)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Employee</td>\n",
       "      <td>No</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>Other qualification</td>\n",
       "      <td>Lower secondary</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "      <td>inapplicable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pidp       pid     n_hidp n_scghq1_dv n_scghq2_dv             n_sclfsato n_health     n_mhcond8     n_mhcond9     n_mhcond6    n_mhcond19    n_mhcond10     n_mhcond4     n_mhcond2    n_mhcond11    n_mhcond12     n_mhcond5    n_mhcond13    n_mhcond14     n_mhcond3    n_mhcond15    n_mhcond16    n_mhcond17    n_mhcond18    n_mhcond97    n_mhcond96   n_sex n_age_dv               n_marstat                                         n_racel_dv  n_nchild_dv  n_hhsize  n_urban_dv                   n_finnow            n_finfut n_ienddaty n_ienddatm n_ienddatd  n_indinub_lw                n_jbstat n_jbhas      n_jbsemp n_julk4wk      n_julkjb       n_jbhad       n_jubgn      n_eprosh      n_jlendm      n_jlendy n_nnmpsp_dv n_nmpsp_dv n_nunmpsp_dv      n_j1none     n_j1still      n_j1mnth      n_j1year   n_j1endmnth   n_j1endyear      n_edtype          n_hiqual_dv           n_isced11_dv      n_qfhigh       n_btec1       n_btec2       n_btec3       n_btec4       n_feend       n_scend\n",
       "0   22445  10127798  276365626          24           7     Somewhat satisfied       No  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  female       37                 Married  british/english/scottish/welsh/northern irish ...            2         4  urban area              Doing alright  or about the same?       2022      April         19           0.0  Paid employment(ft/pt)     Yes      Employee        No  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable        none          1         none  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable               Degree  Masters or equivalent  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable\n",
       "1   29925  10192697  617732426          23          11  Somewhat dissatisfied      Yes  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  female       45                Divorced  british/english/scottish/welsh/northern irish ...            2         3  urban area  Finding it very difficult  or about the same?       2022     August          1           0.0  Paid employment(ft/pt)     Yes      Employee        No  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable        none          1         none  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable               Degree  Masters or equivalent  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable\n",
       "2   76165  10689869  140161626          12           0       Mostly satisfied       No  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  female       39                 Married  british/english/scottish/welsh/northern irish ...            2         4  urban area      Just about getting by          Better off       2022      March          2           0.0  Paid employment(ft/pt)     Yes      Employee        No  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable        none          1         none  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable          A-level etc        Upper secondary  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable\n",
       "3  280165  12430439  753467226          16           5  Somewhat dissatisfied       No  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  female       43                 Married  british/english/scottish/welsh/northern irish ...            1         4  rural area      Just about getting by  or about the same?       2022   December         11           0.0           Self employed      No  inapplicable       Yes  inapplicable  inapplicable           Yes   Very likely  inapplicable  inapplicable        none       none         none  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable             GCSE etc        Lower secondary  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable\n",
       "4  469205  13857142  413725626          16           4       Mostly satisfied       No  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  female       32  Single, nvr marr/civ p  british/english/scottish/welsh/northern irish ...            2         3  urban area              Doing alright  or about the same?       2022        May         10           0.0  Paid employment(ft/pt)     Yes      Employee        No  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable        none          1         none  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  Other qualification        Lower secondary  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable  inapplicable"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5f183c8-72d7-49af-b70c-e808372f106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_counts(\n",
    "    df: pd.DataFrame,\n",
    "    include_cols: list[str] | None = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ËøîÂõû‰∏Ä‰∏™Êï¥Ê¥ÅÁöÑDataFrameÔºå‰ªÖÁªüËÆ°ÊåáÂÆöÂàó‰∏≠ÁöÑÂêÑ‰∏™ÂèñÂÄºÂá∫Áé∞Ê¨°Êï∞„ÄÇ\n",
    "    \n",
    "    ÂèÇÊï∞Ôºö\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Ë¶ÅÂàÜÊûêÁöÑDataFrame„ÄÇ\n",
    "    include_cols : list[str], ÂèØÈÄâ\n",
    "        Âè™ÁªüËÆ°Ëøô‰∫õÂàó„ÄÇÂ¶ÇÊûú‰∏∫ NoneÔºåÂàôÈªòËÆ§ÁªüËÆ°ÊâÄÊúâÂàó„ÄÇ\n",
    "    \n",
    "    ËæìÂá∫ÂàóËØ¥ÊòéÔºö\n",
    "    ----------\n",
    "    - columnÔºöÂàóÂêç\n",
    "    - valueÔºöËØ•Âàó‰∏≠ÁöÑÊüê‰∏™ÂèñÂÄº\n",
    "    - countÔºöËØ•ÂèñÂÄºÂú®ËØ•Âàó‰∏≠Âá∫Áé∞ÁöÑÊ¨°Êï∞\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    # 1Ô∏è‚É£ Á°ÆÂÆöË¶ÅÂ§ÑÁêÜÁöÑÂàó\n",
    "    cols_to_process = include_cols if include_cols else df.columns\n",
    "\n",
    "    # 2Ô∏è‚É£ ÁªüËÆ°ÊØèÂàóÂèñÂÄºÈ¢ëÊ¨°\n",
    "    for col in cols_to_process:\n",
    "        if col not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è Ë≠¶ÂëäÔºö'{col}' ‰∏çÂú® DataFrame Âàó‰∏≠ÔºåÂ∑≤Ë∑≥Ëøá„ÄÇ\")\n",
    "            continue\n",
    "\n",
    "        counts = df[col].value_counts(dropna=False)\n",
    "        for val, cnt in counts.items():\n",
    "            records.append({\n",
    "                'column': col,\n",
    "                'value': val,\n",
    "                'count': cnt\n",
    "            })\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame(records)\n",
    "        .sort_values(['column', 'count'], ascending=[True, False])\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff946499-6a4f-40c0-a317-cd2bddd4e0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        column          value  count\n",
      "0   n_mhcond10   inapplicable  26576\n",
      "1   n_mhcond10  Not mentioned   8502\n",
      "2   n_mhcond10  Yes mentioned    136\n",
      "3   n_mhcond10          proxy    134\n",
      "4   n_mhcond10        refusal     91\n",
      "5   n_mhcond10     don't know     32\n",
      "6   n_mhcond11   inapplicable  26576\n",
      "7   n_mhcond11  Not mentioned   8451\n",
      "8   n_mhcond11  Yes mentioned    187\n",
      "9   n_mhcond11          proxy    134\n",
      "10  n_mhcond11        refusal     91\n",
      "11  n_mhcond11     don't know     32\n",
      "12  n_mhcond12   inapplicable  26576\n",
      "13  n_mhcond12  Not mentioned   8606\n",
      "14  n_mhcond12          proxy    134\n",
      "15  n_mhcond12        refusal     91\n",
      "16  n_mhcond12     don't know     32\n",
      "17  n_mhcond12  Yes mentioned     32\n",
      "18  n_mhcond14   inapplicable  26576\n",
      "19  n_mhcond14  Not mentioned   8564\n",
      "20  n_mhcond14          proxy    134\n",
      "21  n_mhcond14        refusal     91\n",
      "22  n_mhcond14  Yes mentioned     74\n",
      "23  n_mhcond14     don't know     32\n",
      "24  n_mhcond19   inapplicable  26576\n",
      "25  n_mhcond19  Not mentioned   7845\n",
      "26  n_mhcond19  Yes mentioned    793\n",
      "27  n_mhcond19          proxy    134\n",
      "28  n_mhcond19        refusal     91\n",
      "29  n_mhcond19     don't know     32\n",
      "30   n_mhcond2   inapplicable  26576\n",
      "31   n_mhcond2  Not mentioned   7046\n",
      "32   n_mhcond2  Yes mentioned   1592\n",
      "33   n_mhcond2          proxy    134\n",
      "34   n_mhcond2        refusal     91\n",
      "35   n_mhcond2     don't know     32\n",
      "36   n_mhcond5   inapplicable  26576\n",
      "37   n_mhcond5  Not mentioned   8473\n",
      "38   n_mhcond5  Yes mentioned    165\n",
      "39   n_mhcond5          proxy    134\n",
      "40   n_mhcond5        refusal     91\n",
      "41   n_mhcond5     don't know     32\n",
      "42   n_mhcond6   inapplicable  26576\n",
      "43   n_mhcond6  Not mentioned   8341\n",
      "44   n_mhcond6  Yes mentioned    297\n",
      "45   n_mhcond6          proxy    134\n",
      "46   n_mhcond6        refusal     91\n",
      "47   n_mhcond6     don't know     32\n",
      "48   n_mhcond8   inapplicable  26576\n",
      "49   n_mhcond8  Not mentioned   8537\n",
      "50   n_mhcond8          proxy    134\n",
      "51   n_mhcond8  Yes mentioned    101\n",
      "52   n_mhcond8        refusal     91\n",
      "53   n_mhcond8     don't know     32\n",
      "54   n_mhcond9   inapplicable  26576\n",
      "55   n_mhcond9  Not mentioned   7977\n",
      "56   n_mhcond9  Yes mentioned    661\n",
      "57   n_mhcond9          proxy    134\n",
      "58   n_mhcond9        refusal     91\n",
      "59   n_mhcond9     don't know     32\n"
     ]
    }
   ],
   "source": [
    "cols_to_analyze = ['n_mhcond8', 'n_mhcond9', 'n_mhcond6', 'n_mhcond19', 'n_mhcond10', 'n_mhcond14', 'n_mhcond2', 'n_mhcond11', 'n_mhcond12', 'n_mhcond5']\n",
    "\n",
    "result = get_value_counts(df, include_cols=cols_to_analyze)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73650e20-4008-4009-aaa0-70683c96dae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f204798b-404e-4c53-b0dc-7e07fadab827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# ------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "00548bb9-9262-4d33-9539-dd74a9b33565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display setting\n",
    "# ------------------------------------\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ee717d07-c9bb-458f-949c-289acdea4cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.Set File Path\n",
    "# ------------------------------------\n",
    "file_path = \"abspath(root, cfg['ukhls']['indresp_dta'])\"\n",
    "output_dir = abspath(root, cfg['paths']['data_processed'])  # updated\"abspath(root, cfg['paths']['data_processed'])\"\n",
    "output_file_name = \"n_indresp_extracted.csv\"\n",
    "output_path = os.path.join(output_dir, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "30364291-64d8-4bd2-9f5d-b60d440e7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Variables to Extract (use_vars) - All prefixed with 'n_' for Wave 14\n",
    "# ------------------------------------\n",
    "use_vars = [\n",
    "    # 1. Identifiers\n",
    "    \"pidp\",     # Cross-wave person identifier (Core ID)\n",
    "    \"pid\",      # personal identifier (BHPS cohort)\n",
    "    \"n_hidp\",   # Household identifier \n",
    "    \n",
    "    # 2. Mental Health Outcomes\n",
    "    \"n_scghq1_dv\",  # GHQ-12 Score (Likert, 0-36) - Continuous target\n",
    "    \"n_scghq2_dv\",  # GHQ-12 Caseness (Binary) - Classification target\n",
    "    \"n_sclfsato\",   # Satisfaction with life overall (SWB proxy)\n",
    "    \"n_health\",     # Self-reported general health (Control)\n",
    "\n",
    "    # Specific Diagnosed Conditions\n",
    "    \"n_mhcond8\",    # A phobia\n",
    "    \"n_mhcond9\",    # Panic attacks\n",
    "    \"n_mhcond6\",    # Post-traumatic stress disorder (PTSD)\n",
    "    \"n_mhcond19\",   # Any other mental, emotional or neurological problem or condition\n",
    "    \"n_mhcond10\",   # Attention deficit hyperactivity disorder (ADHD)\n",
    "    \"n_mhcond4\",    # Bipolar disorder (or 'manic depression')\n",
    "    \"n_mhcond2\",    # Depression\n",
    "    \"n_mhcond11\",   # Post-natal depression\n",
    "    \"n_mhcond12\",   # Dementia (including Alzheimer's)\n",
    "    \"n_mhcond5\",    # An eating disorder\n",
    "    \"n_mhcond13\",   # Nervous breakdown\n",
    "    \"n_mhcond14\",   # A personality disorder\n",
    "    \"n_mhcond3\",    # Psychosis or schizophrenia\n",
    "    \"n_mhcond15\",   # Obsessive compulsive disorder (OCD)\n",
    "    \"n_mhcond16\",   # Seasonal affective disorder\n",
    "    \"n_mhcond17\",   # Alcohol or drug dependence\n",
    "    \"n_mhcond18\",   # Any other anxiety disorder\n",
    "    \"n_mhcond97\",   # Any other emotional, nervous or psychiatric problem or condition\n",
    "    \"n_mhcond96\",   # None of these (Useful for validation)\n",
    "    \n",
    "    # 3. Demographics and Control Variables\n",
    "    \"n_sex\",        # Sex\n",
    "    \"n_age_dv\",     # Age\n",
    "    \"n_marstat\",    # Marital Status\n",
    "    \"n_racel_dv\",   # Ethnic Group\n",
    "    \"n_nchild_dv\",  # Number of children in household\n",
    "    \"n_hhsize\",     # Household size\n",
    "    \"n_urban_dv\",   # Urban or rural area\n",
    "    \"n_finnow\",     # Subjective financial situation - now\n",
    "    \"n_finfut\",     # Subjective financial situation - future\n",
    "\n",
    "    # Core Controls and Utility Variables (Finance, Date, Weight)\n",
    "    # Household Net Monthly Income (Financial Control)\n",
    "    \"n_ienddaty\",    # Interview Year (CRITICAL for Duration Calculation)\n",
    "    \"n_ienddatm\",    # Interview Month (CRITICAL for Duration Calculation)\n",
    "    \"n_ienddatd\",\n",
    "    \"n_indinub_lw\", # Individual Longitudinal Weight (For Statistical Inference)\n",
    "\n",
    "    # 4. Employment & Unemployment Features\n",
    "    \"n_jbstat\",     # Main economic activity (Current status)\n",
    "    \"n_jbhas\",      # Had paid work last week\n",
    "    \"n_jbsemp\",     # Self-employed\n",
    "    \"n_julk4wk\",    # Looked for work in the last 4 weeks\n",
    "    \"n_julkjb\",     # Would like a regular paid job\n",
    "    \"n_jbhad\",      # Ever had a paid job? (General history)\n",
    "    \"n_jubgn\",      # Able to start work within 2 weeks\n",
    "    \"n_eprosh\",     # Chance starting work within 12 months\n",
    "    \n",
    "    # Last Job End Date (for Current Unemployment Duration)\n",
    "    \"n_jlendm\",     # Month left last job\n",
    "    \"n_jlendy\",     # Year left last job\n",
    "\n",
    "    # Unemployment History\n",
    "    \"n_nnmpsp_dv\",  # No. non-employment spells since last interview\n",
    "    \"n_nmpsp_dv\",   # No. employment spells since last interview\n",
    "    \"n_nunmpsp_dv\", # No. unemployment spells since last interview   \n",
    "\n",
    "    # First Job Status and Duration (Advanced Feature Construction)\n",
    "    \"n_j1none\",     # Still in full-time education / never had a paid job (Used for \"Never Employed\")\n",
    "    \"n_j1still\",    # Still in first job (Used to flag censored duration data)\n",
    "    \"n_j1mnth\",     # First job start month\n",
    "    \"n_j1year\",     # First job start year\n",
    "    \"n_j1endmnth\",  # First job end month\n",
    "    \"n_j1endyear\",  # First job end year\n",
    "    \n",
    "    # 5. Education Variables\n",
    "    \"n_edtype\",     # Type of educational institution attending \n",
    "    \"n_hiqual_dv\",  # Highest educational qualification (UK standard)\n",
    "    \"n_isced11_dv\", # Highest education qualification, short ISCED 2011\n",
    "    \"n_qfhigh\",     # Highest qualification level\n",
    "    \"n_btec1\",      # First certificate or general certificate (below level 2)\n",
    "    \"n_btec2\",      # First diploma or general diploma (level 2)\n",
    "    \"n_btec3\",      # National Certificate or National Diploma level (level 3)\n",
    "    \"n_btec4\",      # Higher level (level 4 or higher)\n",
    "    \n",
    "    # Proxies for Total Years of Education (for CFPS comparability)\n",
    "    \"n_feend\",      # Age finished further education\n",
    "    \"n_scend\"       # Age finished school\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0552a2ff-b9c0-40e4-982f-fe1925407760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted data from /Users/wanderer/Thesis/ModelProject/UKHLS_Data/n_indresp.dta.\n",
      "DataFrame dimensions: (35471, 68)\n",
      "\n",
      "First 5 rows of the data (Preview):\n",
      "\n",
      "Initial cleaning: UKHLS missing codes replaced with NaN.\n"
     ]
    }
   ],
   "source": [
    "# 2.Data Reading Code\n",
    "# ------------------------------------\n",
    "try:\n",
    "    # Attempt to read the Stata file using the specified path and variables\n",
    "    df = pd.read_stata(file_path, \n",
    "                       columns=use_vars, \n",
    "                       convert_categoricals=False)\n",
    "    print(f\"Successfully extracted data from {file_path}.\")\n",
    "    print(f\"DataFrame dimensions: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows of the data (Preview):\")\n",
    "    # print(df.head()) # Uncomment to view the head in your environment\n",
    "    \n",
    "    # Next Step: Missing Value Handling and Feature Engineering\n",
    "    import numpy as np\n",
    "    # Replace UKHLS special missing value codes with NaN\n",
    "    # Common codes: -9 (missing), -8 (inapplicable), -7 (proxy), -2 (refusal), -1 (don't know)\n",
    "    df = df.replace([-9, -8, -7, -2, -1], np.nan)\n",
    "    print(\"\\nInitial cleaning: UKHLS missing codes replaced with NaN.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found. Please confirm the file path is correct: {file_path}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: An issue occurred during data reading (e.g., incorrect variable names or Stata file issue). Details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1fd72a97-ad72-499e-80ba-57fb98f8a202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved the extracteddata to:\n",
      "--> /Users/wanderer/Thesis/ModelProject/Processed_Data/n_indresp_extracted.csv\n"
     ]
    }
   ],
   "source": [
    "# 3.Data Storage (Saving to CSV)\n",
    "# ------------------------------------\n",
    "if df is not None:\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nSuccessfully saved the extracteddata to:\")\n",
    "        print(f\"--> {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file to CSV: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "aa618d0b-fb20-4d3a-a994-eb97dae17ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# ------------------------------------\n",
    "processed_dir = \"abspath(root, cfg['paths']['data_processed'])\"\n",
    "file_name = \"n_indresp_extracted.csv\"\n",
    "file_path = os.path.join(processed_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d51cd51b-1245-4dd2-990e-41aa0633d29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading and Feature Engineering ---\n",
      "Successfully loaded file: /Users/wanderer/Thesis/ModelProject/Processed_Data/n_indresp_extracted.csv. Dimensions: (35471, 68)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Data Loading and Feature Engineering ---\")\n",
    "\n",
    "try:\n",
    "    # Load the extracted and cleaned CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded file: {file_path}. Dimensions: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found. Please confirm the path is correct: {file_path}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8fd6876e-b803-412c-9596-a326d1c5569a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pidp</th>\n",
       "      <th>pid</th>\n",
       "      <th>n_hidp</th>\n",
       "      <th>n_scghq1_dv</th>\n",
       "      <th>n_scghq2_dv</th>\n",
       "      <th>n_sclfsato</th>\n",
       "      <th>n_health</th>\n",
       "      <th>n_mhcond8</th>\n",
       "      <th>n_mhcond9</th>\n",
       "      <th>n_mhcond6</th>\n",
       "      <th>n_mhcond19</th>\n",
       "      <th>n_mhcond10</th>\n",
       "      <th>n_mhcond4</th>\n",
       "      <th>n_mhcond2</th>\n",
       "      <th>n_mhcond11</th>\n",
       "      <th>n_mhcond12</th>\n",
       "      <th>n_mhcond5</th>\n",
       "      <th>n_mhcond13</th>\n",
       "      <th>n_mhcond14</th>\n",
       "      <th>n_mhcond3</th>\n",
       "      <th>n_mhcond15</th>\n",
       "      <th>n_mhcond16</th>\n",
       "      <th>n_mhcond17</th>\n",
       "      <th>n_mhcond18</th>\n",
       "      <th>n_mhcond97</th>\n",
       "      <th>n_mhcond96</th>\n",
       "      <th>n_sex</th>\n",
       "      <th>n_age_dv</th>\n",
       "      <th>n_marstat</th>\n",
       "      <th>n_racel_dv</th>\n",
       "      <th>n_nchild_dv</th>\n",
       "      <th>n_hhsize</th>\n",
       "      <th>n_urban_dv</th>\n",
       "      <th>n_finnow</th>\n",
       "      <th>n_finfut</th>\n",
       "      <th>n_ienddaty</th>\n",
       "      <th>n_ienddatm</th>\n",
       "      <th>n_ienddatd</th>\n",
       "      <th>n_indinub_lw</th>\n",
       "      <th>n_jbstat</th>\n",
       "      <th>n_jbhas</th>\n",
       "      <th>n_jbsemp</th>\n",
       "      <th>n_julk4wk</th>\n",
       "      <th>n_julkjb</th>\n",
       "      <th>n_jbhad</th>\n",
       "      <th>n_jubgn</th>\n",
       "      <th>n_eprosh</th>\n",
       "      <th>n_jlendm</th>\n",
       "      <th>n_jlendy</th>\n",
       "      <th>n_nnmpsp_dv</th>\n",
       "      <th>n_nmpsp_dv</th>\n",
       "      <th>n_nunmpsp_dv</th>\n",
       "      <th>n_j1none</th>\n",
       "      <th>n_j1still</th>\n",
       "      <th>n_j1mnth</th>\n",
       "      <th>n_j1year</th>\n",
       "      <th>n_j1endmnth</th>\n",
       "      <th>n_j1endyear</th>\n",
       "      <th>n_edtype</th>\n",
       "      <th>n_hiqual_dv</th>\n",
       "      <th>n_isced11_dv</th>\n",
       "      <th>n_qfhigh</th>\n",
       "      <th>n_btec1</th>\n",
       "      <th>n_btec2</th>\n",
       "      <th>n_btec3</th>\n",
       "      <th>n_btec4</th>\n",
       "      <th>n_feend</th>\n",
       "      <th>n_scend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22445</td>\n",
       "      <td>10127798.0</td>\n",
       "      <td>276365626</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29925</td>\n",
       "      <td>10192697.0</td>\n",
       "      <td>617732426</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76165</td>\n",
       "      <td>10689869.0</td>\n",
       "      <td>140161626</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>280165</td>\n",
       "      <td>12430439.0</td>\n",
       "      <td>753467226</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>469205</td>\n",
       "      <td>13857142.0</td>\n",
       "      <td>413725626</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pidp         pid     n_hidp  n_scghq1_dv  n_scghq2_dv  n_sclfsato  n_health  n_mhcond8  n_mhcond9  n_mhcond6  n_mhcond19  n_mhcond10  n_mhcond4  n_mhcond2  n_mhcond11  n_mhcond12  n_mhcond5  n_mhcond13  n_mhcond14  n_mhcond3  n_mhcond15  n_mhcond16  n_mhcond17  n_mhcond18  n_mhcond97  n_mhcond96  n_sex  n_age_dv  n_marstat  n_racel_dv  n_nchild_dv  n_hhsize  n_urban_dv  n_finnow  n_finfut  n_ienddaty  n_ienddatm  n_ienddatd  n_indinub_lw  n_jbstat  n_jbhas  n_jbsemp  n_julk4wk  n_julkjb  n_jbhad  n_jubgn  n_eprosh  n_jlendm  n_jlendy  n_nnmpsp_dv  n_nmpsp_dv  n_nunmpsp_dv  n_j1none  n_j1still  n_j1mnth  n_j1year  n_j1endmnth  n_j1endyear  n_edtype  n_hiqual_dv  n_isced11_dv  n_qfhigh  n_btec1  n_btec2  n_btec3  n_btec4  n_feend  n_scend\n",
       "0   22445  10127798.0  276365626         24.0          7.0         5.0       2.0        NaN        NaN        NaN         NaN         NaN        NaN        NaN         NaN         NaN        NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN    2.0      37.0        2.0         1.0            2         4         1.0       2.0       3.0      2022.0         4.0        19.0           0.0       2.0      1.0       1.0        2.0       NaN      NaN      NaN       NaN       NaN       NaN          0.0         1.0           0.0       NaN        NaN       NaN       NaN          NaN          NaN       NaN          1.0           7.0       NaN      NaN      NaN      NaN      NaN      NaN      NaN\n",
       "1   29925  10192697.0  617732426         23.0         11.0         3.0       1.0        NaN        NaN        NaN         NaN         NaN        NaN        NaN         NaN         NaN        NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN    2.0      45.0        5.0         1.0            2         3         1.0       5.0       3.0      2022.0         8.0         1.0           0.0       2.0      1.0       1.0        2.0       NaN      NaN      NaN       NaN       NaN       NaN          0.0         1.0           0.0       NaN        NaN       NaN       NaN          NaN          NaN       NaN          1.0           7.0       NaN      NaN      NaN      NaN      NaN      NaN      NaN\n",
       "2   76165  10689869.0  140161626         12.0          0.0         6.0       2.0        NaN        NaN        NaN         NaN         NaN        NaN        NaN         NaN         NaN        NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN    2.0      39.0        2.0         1.0            2         4         1.0       3.0       1.0      2022.0         3.0         2.0           0.0       2.0      1.0       1.0        2.0       NaN      NaN      NaN       NaN       NaN       NaN          0.0         1.0           0.0       NaN        NaN       NaN       NaN          NaN          NaN       NaN          3.0           3.0       NaN      NaN      NaN      NaN      NaN      NaN      NaN\n",
       "3  280165  12430439.0  753467226         16.0          5.0         3.0       2.0        NaN        NaN        NaN         NaN         NaN        NaN        NaN         NaN         NaN        NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN    2.0      43.0        2.0         1.0            1         4         2.0       3.0       3.0      2022.0        12.0        11.0           0.0       1.0      2.0       NaN        1.0       NaN      NaN      1.0       1.0       NaN       NaN          0.0         0.0           0.0       NaN        NaN       NaN       NaN          NaN          NaN       NaN          4.0           2.0       NaN      NaN      NaN      NaN      NaN      NaN      NaN\n",
       "4  469205  13857142.0  413725626         16.0          4.0         6.0       2.0        NaN        NaN        NaN         NaN         NaN        NaN        NaN         NaN         NaN        NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN    2.0      32.0        1.0         1.0            2         3         1.0       2.0       3.0      2022.0         5.0        10.0           0.0       2.0      1.0       1.0        2.0       NaN      NaN      NaN       NaN       NaN       NaN          0.0         1.0           0.0       NaN        NaN       NaN       NaN          NaN          NaN       NaN          5.0           2.0       NaN      NaN      NaN      NaN      NaN      NaN      NaN"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6173fcc2-15df-4dab-9976-8d4a172ebe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2. Exploratory Data Analysis: Initial Inspection ---\n",
      "\n",
      "[1] Data Dimensions (Rows, Columns): (35471, 68)\n",
      "\n",
      "[2] Head of the Dataset:\n",
      "\n",
      "[3] DataFrame Information (Data Types and Missing Value Overview):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35471 entries, 0 to 35470\n",
      "Data columns (total 68 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   pidp          35471 non-null  int64  \n",
      " 1   pid           5814 non-null   float64\n",
      " 2   n_hidp        35471 non-null  int64  \n",
      " 3   n_scghq1_dv   34114 non-null  float64\n",
      " 4   n_scghq2_dv   34114 non-null  float64\n",
      " 5   n_sclfsato    34383 non-null  float64\n",
      " 6   n_health      35261 non-null  float64\n",
      " 7   n_mhcond8     8638 non-null   float64\n",
      " 8   n_mhcond9     8638 non-null   float64\n",
      " 9   n_mhcond6     8638 non-null   float64\n",
      " 10  n_mhcond19    8638 non-null   float64\n",
      " 11  n_mhcond10    8638 non-null   float64\n",
      " 12  n_mhcond4     8638 non-null   float64\n",
      " 13  n_mhcond2     8638 non-null   float64\n",
      " 14  n_mhcond11    8638 non-null   float64\n",
      " 15  n_mhcond12    8638 non-null   float64\n",
      " 16  n_mhcond5     8638 non-null   float64\n",
      " 17  n_mhcond13    8638 non-null   float64\n",
      " 18  n_mhcond14    8638 non-null   float64\n",
      " 19  n_mhcond3     8638 non-null   float64\n",
      " 20  n_mhcond15    8638 non-null   float64\n",
      " 21  n_mhcond16    8638 non-null   float64\n",
      " 22  n_mhcond17    8638 non-null   float64\n",
      " 23  n_mhcond18    8638 non-null   float64\n",
      " 24  n_mhcond97    8638 non-null   float64\n",
      " 25  n_mhcond96    8638 non-null   float64\n",
      " 26  n_sex         35464 non-null  float64\n",
      " 27  n_age_dv      35445 non-null  float64\n",
      " 28  n_marstat     35090 non-null  float64\n",
      " 29  n_racel_dv    35271 non-null  float64\n",
      " 30  n_nchild_dv   35471 non-null  int64  \n",
      " 31  n_hhsize      35471 non-null  int64  \n",
      " 32  n_urban_dv    35460 non-null  float64\n",
      " 33  n_finnow      35022 non-null  float64\n",
      " 34  n_finfut      34631 non-null  float64\n",
      " 35  n_ienddaty    35057 non-null  float64\n",
      " 36  n_ienddatm    35057 non-null  float64\n",
      " 37  n_ienddatd    35057 non-null  float64\n",
      " 38  n_indinub_lw  35471 non-null  float64\n",
      " 39  n_jbstat      35341 non-null  float64\n",
      " 40  n_jbhas       35196 non-null  float64\n",
      " 41  n_jbsemp      19085 non-null  float64\n",
      " 42  n_julk4wk     35105 non-null  float64\n",
      " 43  n_julkjb      14600 non-null  float64\n",
      " 44  n_jbhad       3581 non-null   float64\n",
      " 45  n_jubgn       3256 non-null   float64\n",
      " 46  n_eprosh      15384 non-null  float64\n",
      " 47  n_jlendm      2462 non-null   float64\n",
      " 48  n_jlendy      2848 non-null   float64\n",
      " 49  n_nnmpsp_dv   23980 non-null  float64\n",
      " 50  n_nmpsp_dv    23965 non-null  float64\n",
      " 51  n_nunmpsp_dv  23958 non-null  float64\n",
      " 52  n_j1none      3726 non-null   float64\n",
      " 53  n_j1still     7755 non-null   float64\n",
      " 54  n_j1mnth      6651 non-null   float64\n",
      " 55  n_j1year      7399 non-null   float64\n",
      " 56  n_j1endmnth   5484 non-null   float64\n",
      " 57  n_j1endyear   6404 non-null   float64\n",
      " 58  n_edtype      1620 non-null   float64\n",
      " 59  n_hiqual_dv   34398 non-null  float64\n",
      " 60  n_isced11_dv  34371 non-null  float64\n",
      " 61  n_qfhigh      8007 non-null   float64\n",
      " 62  n_btec1       791 non-null    float64\n",
      " 63  n_btec2       791 non-null    float64\n",
      " 64  n_btec3       791 non-null    float64\n",
      " 65  n_btec4       791 non-null    float64\n",
      " 66  n_feend       5569 non-null   float64\n",
      " 67  n_scend       8143 non-null   float64\n",
      "dtypes: float64(64), int64(4)\n",
      "memory usage: 18.4 MB\n",
      "\n",
      "[4] Descriptive Statistics for Numerical Variables (df.describe()):\n",
      "                count          mean           std         min           25%          50%           75%           max\n",
      "pidp          35471.0  8.098408e+08  4.736139e+08     22445.0  4.086355e+08  768691049.0  1.224689e+09  1.668462e+09\n",
      "pid            5814.0  7.104199e+07  5.124105e+07  10019057.0  1.585221e+07   94733620.0  1.185919e+08  1.893054e+08\n",
      "n_hidp        35471.0  8.095099e+08  4.736921e+08  68006826.0  4.099346e+08  754194826.0  1.226251e+09  1.668462e+09\n",
      "n_scghq1_dv   34114.0  1.186144e+01  5.876782e+00         0.0  8.000000e+00         11.0  1.400000e+01  3.600000e+01\n",
      "n_scghq2_dv   34114.0  2.020549e+00  3.278254e+00         0.0  0.000000e+00          0.0  3.000000e+00  1.200000e+01\n",
      "n_sclfsato    34383.0  5.088387e+00  1.440403e+00         1.0  4.000000e+00          6.0  6.000000e+00  7.000000e+00\n",
      "n_health      35261.0  1.642239e+00  4.793480e-01         1.0  1.000000e+00          2.0  2.000000e+00  2.000000e+00\n",
      "n_mhcond8      8638.0  1.169252e-02  1.075042e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond9      8638.0  7.652234e-02  2.658474e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond6      8638.0  3.438296e-02  1.822213e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond19     8638.0  9.180366e-02  2.887653e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond10     8638.0  1.574439e-02  1.244921e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond4      8638.0  7.409122e-03  8.576176e-02         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond2      8638.0  1.843019e-01  3.877527e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond11     8638.0  2.164853e-02  1.455415e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond12     8638.0  3.704561e-03  6.075578e-02         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond5      8638.0  1.910164e-02  1.368902e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond13     8638.0  1.759667e-02  1.314877e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond14     8638.0  8.566798e-03  9.216502e-02         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond3      8638.0  4.862237e-03  6.956404e-02         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond15     8638.0  1.597592e-02  1.253894e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond16     8638.0  1.076638e-02  1.032071e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond17     8638.0  9.145636e-03  9.520001e-02         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond18     8638.0  2.801574e-02  1.650273e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond97     8638.0  1.678629e-02  1.284773e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_mhcond96     8638.0  7.211160e-01  4.484763e-01         0.0  0.000000e+00          1.0  1.000000e+00  1.000000e+00\n",
      "n_sex         35464.0  1.558510e+00  4.965718e-01         1.0  1.000000e+00          2.0  2.000000e+00  2.000000e+00\n",
      "n_age_dv      35445.0  5.088596e+01  1.883227e+01        16.0  3.600000e+01         52.0  6.600000e+01  1.010000e+02\n",
      "n_marstat     35090.0  2.265460e+00  1.472716e+00         1.0  1.000000e+00          2.0  2.000000e+00  9.000000e+00\n",
      "n_racel_dv    35271.0  2.788070e+00  5.733365e+00         1.0  1.000000e+00          1.0  1.000000e+00  9.700000e+01\n",
      "n_nchild_dv   35471.0  3.666939e-01  7.817234e-01         0.0  0.000000e+00          0.0  0.000000e+00  7.000000e+00\n",
      "n_hhsize      35471.0  2.768628e+00  1.459288e+00         1.0  2.000000e+00          2.0  4.000000e+00  1.300000e+01\n",
      "n_urban_dv    35460.0  1.257699e+00  4.373734e-01         1.0  1.000000e+00          1.0  2.000000e+00  2.000000e+00\n",
      "n_finnow      35022.0  2.187711e+00  9.863578e-01         1.0  1.000000e+00          2.0  3.000000e+00  5.000000e+00\n",
      "n_finfut      34631.0  2.326759e+00  7.766516e-01         1.0  2.000000e+00          3.0  3.000000e+00  3.000000e+00\n",
      "n_ienddaty    35057.0  2.022510e+03  5.545459e-01      2022.0  2.022000e+03       2022.0  2.023000e+03  2.024000e+03\n",
      "n_ienddatm    35057.0  6.226431e+00  3.422841e+00         1.0  3.000000e+00          6.0  9.000000e+00  1.200000e+01\n",
      "n_ienddatd    35057.0  1.560413e+01  9.122005e+00         1.0  8.000000e+00         15.0  2.400000e+01  3.100000e+01\n",
      "n_indinub_lw  35471.0  3.708945e-01  6.669299e-01         0.0  0.000000e+00          0.0  6.414796e-01  6.085530e+00\n",
      "n_jbstat      35341.0  3.951869e+00  8.772393e+00         1.0  2.000000e+00          2.0  4.000000e+00  9.700000e+01\n",
      "n_jbhas       35196.0  1.485112e+00  4.997854e-01         1.0  1.000000e+00          1.0  2.000000e+00  2.000000e+00\n",
      "n_jbsemp      19085.0  1.116898e+00  3.213072e-01         1.0  1.000000e+00          1.0  1.000000e+00  2.000000e+00\n",
      "n_julk4wk     35105.0  1.942401e+00  2.329862e-01         1.0  2.000000e+00          2.0  2.000000e+00  2.000000e+00\n",
      "n_julkjb      14600.0  1.831644e+00  3.741951e-01         1.0  2.000000e+00          2.0  2.000000e+00  2.000000e+00\n",
      "n_jbhad        3581.0  1.138230e+00  3.451890e-01         1.0  1.000000e+00          1.0  1.000000e+00  2.000000e+00\n",
      "n_jubgn        3256.0  1.549447e+00  4.976254e-01         1.0  1.000000e+00          2.0  2.000000e+00  2.000000e+00\n",
      "n_eprosh      15384.0  3.442343e+00  9.698322e-01         1.0  3.000000e+00          4.0  4.000000e+00  4.000000e+00\n",
      "n_jlendm       2462.0  6.460601e+00  3.257828e+00         1.0  4.000000e+00          6.0  9.000000e+00  1.200000e+01\n",
      "n_jlendy       2848.0  2.011327e+03  1.101381e+01      1953.0  2.006000e+03       2014.0  2.020000e+03  2.024000e+03\n",
      "n_nnmpsp_dv   23980.0  4.537531e-01  4.984530e-01         0.0  0.000000e+00          0.0  1.000000e+00  3.000000e+00\n",
      "n_nmpsp_dv    23965.0  6.341748e-01  4.919571e-01         0.0  0.000000e+00          1.0  1.000000e+00  4.000000e+00\n",
      "n_nunmpsp_dv  23958.0  5.714166e-02  2.321180e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_j1none       3726.0  2.742619e+00  6.122183e-01         1.0  3.000000e+00          3.0  3.000000e+00  3.000000e+00\n",
      "n_j1still      7755.0  1.910896e+00  2.849119e-01         1.0  2.000000e+00          2.0  2.000000e+00  2.000000e+00\n",
      "n_j1mnth       6651.0  6.914449e+00  2.709558e+00         1.0  6.000000e+00          7.0  9.000000e+00  1.200000e+01\n",
      "n_j1year       7399.0  1.990988e+03  1.930759e+01      1931.0  1.975000e+03       1992.0  2.007000e+03  2.024000e+03\n",
      "n_j1endmnth    5484.0  6.659555e+00  3.148446e+00         1.0  4.000000e+00          7.0  9.000000e+00  1.200000e+01\n",
      "n_j1endyear    6404.0  1.994037e+03  1.857124e+01      1938.0  1.979000e+03       1996.0  2.010000e+03  2.024000e+03\n",
      "n_edtype       1620.0  3.655556e+00  1.577286e+00         1.0  2.000000e+00          5.0  5.000000e+00  5.000000e+00\n",
      "n_hiqual_dv   34398.0  3.016047e+00  2.142177e+00         1.0  1.000000e+00          3.0  4.000000e+00  9.000000e+00\n",
      "n_isced11_dv  34371.0  1.162433e+01  2.493999e+01         2.0  2.000000e+00          6.0  6.000000e+00  9.600000e+01\n",
      "n_qfhigh       8007.0  2.290621e+01  2.924648e+01         1.0  6.000000e+00         12.0  2.000000e+01  9.600000e+01\n",
      "n_btec1         791.0  2.035398e-01  4.028853e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_btec2         791.0  2.376738e-01  4.259276e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_btec3         791.0  5.347661e-01  4.991054e-01         0.0  0.000000e+00          1.0  1.000000e+00  1.000000e+00\n",
      "n_btec4         791.0  1.327434e-01  3.395118e-01         0.0  0.000000e+00          0.0  0.000000e+00  1.000000e+00\n",
      "n_feend        5569.0  2.238660e+01  6.544013e+00        16.0  1.800000e+01         21.0  2.300000e+01  7.200000e+01\n",
      "n_scend        8143.0  1.662532e+01  1.289084e+00        10.0  1.600000e+01         16.0  1.800000e+01  3.400000e+01\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 2. Exploratory Data Analysis: Initial Inspection ---\")\n",
    "\n",
    "# 1. Check the dimensions of the DataFrame (Rows, Columns)\n",
    "print(f\"\\n[1] Data Dimensions (Rows, Columns): {df.shape}\")\n",
    "\n",
    "# 2. View the first 5 rows to get a quick look at the data structure\n",
    "print(\"\\n[2] Head of the Dataset:\")\n",
    "# print(df.head())\n",
    "\n",
    "# 3. Get a concise summary of the DataFrame, including the number of non-null values and data types\n",
    "print(\"\\n[3] DataFrame Information (Data Types and Missing Value Overview):\")\n",
    "# The output of df.info() will show non-null counts, which helps identify missing data.\n",
    "df.info()\n",
    "\n",
    "# 4. Descriptive statistics for numerical columns: count, mean, std, min, max, quartiles\n",
    "print(\"\\n[4] Descriptive Statistics for Numerical Variables (df.describe()):\")\n",
    "# print(df.describe())\n",
    "\n",
    "# 5. Descriptive statistics for categorical (object) columns: count, unique, top, freq\n",
    "# 1. Generate the descriptive statistics for object columns\n",
    "numerical_stats = df.describe(include=['number'])\n",
    "\n",
    "# 2. Transpose the result using .T\n",
    "transposed_stats = numerical_stats.T\n",
    "\n",
    "# 3. Print the transposed DataFrame\n",
    "print(transposed_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1eaf7-f43c-4d4c-ac59-dbcbaa1f3d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06f5c4b1-f6a8-4f79-99e2-4d2177693117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54c31bc6-f4dd-4bdb-b711-d415d38a7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "# Set font for better compatibility (adjust if specific font is needed)\n",
    "plt.rcParams['figure.figsize'] = (15, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0eaa37f2-0625-4f25-852c-d7e5b704efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Configuration\n",
    "# ------------------------------------\n",
    "processed_dir = \"abspath(root, cfg['paths']['data_processed'])\"\n",
    "file_name = \"n_indresp_extracted.csv\"\n",
    "file_path = os.path.join(processed_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9eb69df0-9813-44f6-9e57-d236519b19b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from: /Users/wanderer/Thesis/ModelProject/Processed_Data/n_indresp_extracted.csv\n"
     ]
    }
   ],
   "source": [
    "# --- GHQ Items List and UKHLS Missing Codes ---\n",
    "UKHLS_MISSING_CODES = [-9.0, -8.0, -7.0, -2.0, -1.0]\n",
    "\n",
    "# --- Helper Function 1: Clean Missing Codes (for float conversion) ---\n",
    "def clean_missing_codes(df, cols):\n",
    "    \"\"\"Replaces UKHLS special negative codes with NaN and ensures float type.\"\"\"\n",
    "    # Note: Use errors='ignore' in astype(float) if the column might contain non-numeric strings\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(UKHLS_MISSING_CODES, np.nan).astype(float, errors='ignore')\n",
    "    return df\n",
    "\n",
    "# --- Helper Function 2: Clean and Convert to Nullable Integer ---\n",
    "def clean_and_convert_to_nullable_int(df, cols):\n",
    "    \"\"\"Converts UKHLS missing codes to NaN and converts to Pandas' nullable Int64.\"\"\"\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(UKHLS_MISSING_CODES, np.nan)\n",
    "            try:\n",
    "                # Convert to nullable Int64 (supports NaN)\n",
    "                df[col] = df[col].astype('Int64')\n",
    "            except Exception:\n",
    "                # Fallback to float if conversion to Int64 fails\n",
    "                df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "# ==============================================================================\n",
    "# 0. Load Data and Initial Cleaning\n",
    "# ==============================================================================\n",
    "try:\n",
    "    df = pd.read_csv(file_path) \n",
    "    print(f\"Successfully loaded data from: {file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}. Please check the path and filename.\")\n",
    "    exit() \n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during file loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Apply base cleaning to all columns that might contain UKHLS codes\n",
    "all_cols_to_clean = [col for col in df.columns if col not in ['pidp', 'pid', 'n_hidp']]\n",
    "df = clean_missing_codes(df, all_cols_to_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "96e0a0eb-5e90-45e5-99bb-9aee511e8e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_16666/1748425218.py:34: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Associated MH Issue' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['primary_mh_indicator'] == 1.0,\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_16666/1748425218.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Never Employed' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['n_j1none'] == 2, 'early_career_unemp_risk'] = 'Never Employed'\n",
      "/var/folders/rr/h4pmcm_12sd99gy5v8tk3dlc0000gn/T/ipykernel_16666/1748425218.py:116: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'High Stability: No change reported' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['total_spells_since_last_int'] == 0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Engineering 1: Mental Health Status Completed ---\n",
      "mental_health_status\n",
      "Missing/Inapplicable    26833\n",
      "No Reported MH Issue     6229\n",
      "Associated MH Issue      2054\n",
      "Other MH Issue            355\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Feature Engineering 2: Early Career Unemployment Risk Completed ---\n",
      "early_career_unemp_risk\n",
      "Missing/Inapplicable                29180\n",
      "Stable Start: Left Long-Term Job     4078\n",
      "Early Failure: Quick Exit            1171\n",
      "Stable Start: Still in First Job      691\n",
      "Never Employed                        275\n",
      "Extended Unemp after Quick Exit        76\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Feature Engineering 3: Employment Stability Completed ---\n",
      "employment_stability_level\n",
      "Missing/Inapplicable                           14142\n",
      "Moderate Stability: Single Employment Spell    12422\n",
      "Low Stability: Non-Employment Spells Only       8269\n",
      "High Stability: No change reported               606\n",
      "High Volatility: Frequent Changes                 32\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Feature Engineering A: GHQ-12 Score (Aggregated) Completed ---\n",
      "--- Feature Engineering B: Education Level (Ordered) Created ---\n",
      "\n",
      "--- Feature Engineering B.1: Vocational Qualification Indicator Completed ---\n",
      "has_vocational_qual\n",
      "<NA>    34680\n",
      "1         791\n",
      "Name: count, dtype: Int64\n",
      "--- Feature Engineering C: Control Variables (Binary/OHE) Completed ---\n",
      "\n",
      "--- Feature Engineering D: Advanced Employment Features (Fixed) Completed ---\n",
      "       is_currently_unemployed  last_unemployment_duration_months  \\\n",
      "count                  35341.0                        2305.000000   \n",
      "mean                  0.042132                         130.735792   \n",
      "std                   0.200894                         126.760077   \n",
      "min                        0.0                           0.000000   \n",
      "25%                        0.0                          35.000000   \n",
      "50%                        0.0                          95.000000   \n",
      "75%                        0.0                         188.000000   \n",
      "max                        1.0                         836.000000   \n",
      "\n",
      "       marginally_attached  \n",
      "count              14600.0  \n",
      "mean              0.168356  \n",
      "std               0.374195  \n",
      "min                    0.0  \n",
      "25%                    0.0  \n",
      "50%                    0.0  \n",
      "75%                    0.0  \n",
      "max                    1.0  \n",
      "\n",
      "--- Feature Engineering E: Demographic and Control Variables (Fixed) Completed ---\n",
      "\n",
      "--- Feature Engineering E: Demographic and Control Variables Completed ---\n",
      "\n",
      "--- ALL Feature Engineering Stages Complete. Optimized data saved to: /Users/wanderer/Thesis/ModelProject/Processed_Data/n_indresp_FINAL_FEATURES_OPTIMIZED.csv ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. Feature Engineering: Mental Health Status (Outcome Variable) - USER CODE\n",
    "# ==============================================================================\n",
    "\n",
    "# 1.1 Define Primary (Associated) and Secondary (Other) MH Variable Groups\n",
    "primary_mh_vars = [\n",
    "    'n_mhcond19', 'n_mhcond2', 'n_mhcond5', 'n_mhcond13', \n",
    "    'n_mhcond17', 'n_mhcond18', 'n_mhcond97'\n",
    "]\n",
    "secondary_mh_vars_no_none = [ # Exclude n_mhcond96 from the positive check for now\n",
    "    'n_mhcond8', 'n_mhcond9', 'n_mhcond6', 'n_mhcond10', \n",
    "    'n_mhcond4', 'n_mhcond11', 'n_mhcond12', 'n_mhcond14', \n",
    "    'n_mhcond3', 'n_mhcond15', 'n_mhcond16'\n",
    "]\n",
    "all_positive_mh_vars = primary_mh_vars + secondary_mh_vars_no_none\n",
    "all_mh_vars_including_96 = all_positive_mh_vars + ['n_mhcond96']\n",
    "\n",
    "# 1.2 Clean and Prepare MH Data\n",
    "# Convert UKHLS missing codes to NaN and ensure float type for calculation.\n",
    "# This cleaning is handled by the initial clean_missing_codes call, but we ensure float type.\n",
    "df[all_mh_vars_including_96] = df[all_mh_vars_including_96].astype(float)\n",
    "\n",
    "# 1.3 Create Indicator Variables (1.0 = Yes mentioned, NaN = Missing, 0.0 = Not mentioned)\n",
    "df['primary_mh_indicator'] = df[primary_mh_vars].max(axis=1, skipna=True)\n",
    "df['secondary_mh_indicator'] = df[secondary_mh_vars_no_none].max(axis=1, skipna=True)\n",
    "df['any_mh_problem'] = df[all_positive_mh_vars].max(axis=1, skipna=True)\n",
    "\n",
    "# 1.4 Construct the Final Categorical Variable 'mental_health_status'\n",
    "df['mental_health_status'] = np.nan # Default to NaN\n",
    "\n",
    "# --- Prioritization Logic ---\n",
    "\n",
    "# 1. Associated MH Issue (any primary condition is 1.0)\n",
    "df.loc[df['primary_mh_indicator'] == 1.0, \n",
    "       'mental_health_status'] = 'Associated MH Issue'\n",
    "\n",
    "# 2. Other MH Issue (secondary condition is 1.0 AND no primary condition is 1.0)\n",
    "df.loc[(df['primary_mh_indicator'] == 0.0) & (df['secondary_mh_indicator'] == 1.0), \n",
    "       'mental_health_status'] = 'Other MH Issue'\n",
    "\n",
    "# 3. No Reported MH Issue (Max of all positive MH vars must be 0.0)\n",
    "mh_answered_but_no_problem = (df['any_mh_problem'] == 0.0)\n",
    "df.loc[mh_answered_but_no_problem, \n",
    "       'mental_health_status'] = 'No Reported MH Issue'\n",
    "\n",
    "# 4. Missing/Inapplicable \n",
    "df['mental_health_status'] = df['mental_health_status'].fillna('Missing/Inapplicable')\n",
    "\n",
    "# 1.5 Clean up auxiliary columns\n",
    "df = df.drop(columns=['primary_mh_indicator', 'secondary_mh_indicator', 'any_mh_problem'])\n",
    "print(\"\\n--- Feature Engineering 1: Mental Health Status Completed ---\")\n",
    "print(df['mental_health_status'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Feature Engineering: Early Career Unemployment Risk - USER CODE\n",
    "# ==============================================================================\n",
    "\n",
    "# 2.1 Clean and Prepare Variables\n",
    "time_vars = ['n_j1mnth', 'n_j1year', 'n_j1endmnth', 'n_j1endyear', 'n_j1none', 'n_j1still', 'n_jbstat']\n",
    "df = clean_and_convert_to_nullable_int(df, time_vars)\n",
    "\n",
    "# 2.2 Define Threshold for \"Quick Exit\"\n",
    "QUICK_EXIT_THRESHOLD_MONTHS = 12\n",
    "\n",
    "# 2.3 Calculate First Job Duration (in months)\n",
    "df['first_job_duration_months'] = np.where(\n",
    "    df['n_j1year'].notna() & df['n_j1endyear'].notna() & df['n_j1mnth'].notna() & df['n_j1endmnth'].notna(),\n",
    "    (df['n_j1endyear'] - df['n_j1year']) * 12 + (df['n_j1endmnth'] - df['n_j1mnth']),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# 2.4 Construct the Early Career Risk Variable\n",
    "df['early_career_unemp_risk'] = np.nan \n",
    "\n",
    "# Code 2: Never had a paid job\n",
    "df.loc[df['n_j1none'] == 2, 'early_career_unemp_risk'] = 'Never Employed'\n",
    "\n",
    "# Code 1: Still in first job\n",
    "df.loc[df['n_j1still'] == 1, 'early_career_unemp_risk'] = 'Stable Start: Still in First Job'\n",
    "\n",
    "# A. Early Failure: Duration < Threshold\n",
    "df.loc[(df['n_j1still'] == 2) & (df['first_job_duration_months'] < QUICK_EXIT_THRESHOLD_MONTHS),\n",
    "       'early_career_unemp_risk'] = 'Early Failure: Quick Exit'\n",
    "\n",
    "# B. Stable Start: Left Long-Term Job: Duration >= Threshold\n",
    "df.loc[(df['n_j1still'] == 2) & (df['first_job_duration_months'] >= QUICK_EXIT_THRESHOLD_MONTHS),\n",
    "       'early_career_unemp_risk'] = 'Stable Start: Left Long-Term Job'\n",
    "\n",
    "# C. Extended Unemployment after Early Failure (Unemployed now, n_jbstat=3)\n",
    "df.loc[(df['early_career_unemp_risk'] == 'Early Failure: Quick Exit') & (df['n_jbstat'] == 3),\n",
    "       'early_career_unemp_risk'] = 'Extended Unemp after Quick Exit'\n",
    "\n",
    "# 2.5 Finalize Missing / Inapplicable\n",
    "df['early_career_unemp_risk'] = df['early_career_unemp_risk'].fillna('Missing/Inapplicable')\n",
    "       \n",
    "print(\"\\n--- Feature Engineering 2: Early Career Unemployment Risk Completed ---\")\n",
    "print(df['early_career_unemp_risk'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Feature Engineering: Employment Volatility (Employment Stability) - USER CODE\n",
    "# ==============================================================================\n",
    "\n",
    "# 3.1 Clean and Prepare Variables\n",
    "volatility_vars = ['n_nnmpsp_dv', 'n_nmpsp_dv', 'n_nunmpsp_dv']\n",
    "df = clean_and_convert_to_nullable_int(df, volatility_vars)\n",
    "\n",
    "# 3.2 Construct Total Spells (Job changes/state changes)\n",
    "df['total_spells_since_last_int'] = df['n_nnmpsp_dv'] + df['n_nmpsp_dv']\n",
    "\n",
    "# 3.3 Construct Employment Stability Index\n",
    "df['employment_stability_level'] = np.nan\n",
    "\n",
    "# Case 1: Highest Stability - Zero Spells\n",
    "df.loc[df['total_spells_since_last_int'] == 0, \n",
    "       'employment_stability_level'] = 'High Stability: No change reported'\n",
    "       \n",
    "# Case 2: Moderate Stability - One Employment Spell, Zero Non-employment Spells\n",
    "df.loc[(df['n_nmpsp_dv'] == 1) & (df['n_nnmpsp_dv'] == 0), \n",
    "       'employment_stability_level'] = 'Moderate Stability: Single Employment Spell'\n",
    "       \n",
    "# Case 3: Low Stability - Single Non-Employment Spell \n",
    "df.loc[(df['n_nmpsp_dv'] == 0) & (df['n_nnmpsp_dv'] >= 1), \n",
    "       'employment_stability_level'] = 'Low Stability: Non-Employment Spells Only'\n",
    "       \n",
    "# Case 4: High Volatility - Multiple Spells (Threshold >= 3)\n",
    "df.loc[df['total_spells_since_last_int'] >= 3, \n",
    "       'employment_stability_level'] = 'High Volatility: Frequent Changes'\n",
    "\n",
    "# 3.4 Finalize Missing/Inapplicable\n",
    "df['employment_stability_level'] = df['employment_stability_level'].fillna('Missing/Inapplicable')\n",
    "\n",
    "df['unemployment_spells_count'] = df['n_nunmpsp_dv']\n",
    "df = df.drop(columns=['total_spells_since_last_int'])\n",
    "\n",
    "print(\"\\n--- Feature Engineering 3: Employment Stability Completed ---\")\n",
    "print(df['employment_stability_level'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# A. GHQ-12 Score (Using Aggregated Variable n_scghq2_dv)\n",
    "# ==============================================================================\n",
    "ghq_caseness_var = 'n_scghq2_dv'\n",
    "\n",
    "# Clean and rename n_scghq2_dv (already cleaned to float by the initial step)\n",
    "df['ghq12_continuous_score'] = df[ghq_caseness_var].astype(float) \n",
    "\n",
    "print(\"\\n--- Feature Engineering A: GHQ-12 Score (Aggregated) Completed ---\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# B. Education Feature: Ordered Categorical Variable (ISCED-11 based)\n",
    "# ==============================================================================\n",
    "\n",
    "isced_vars = ['n_isced11_dv', 'n_hiqual_dv']\n",
    "df = clean_and_convert_to_nullable_int(df, isced_vars) # Convert ISCED to Int for mapping\n",
    "\n",
    "isced_var = 'n_isced11_dv'\n",
    "education_mapping = {\n",
    "    # Low Education: ISCED 0-3 (Codes 9, 2, 3)\n",
    "    9: 'Low_Education',   \n",
    "    2: 'Low_Education',   \n",
    "    3: 'Low_Education',   \n",
    "    \n",
    "    # Medium Education: ISCED 4-5 (Codes 4, 5)\n",
    "    4: 'Medium_Education', \n",
    "    5: 'Medium_Education', \n",
    "    \n",
    "    # High Education: ISCED 6-8 (Codes 6, 7, 8)\n",
    "    6: 'High_Education',   \n",
    "    7: 'High_Education',   \n",
    "    8: 'High_Education',   \n",
    "}\n",
    "\n",
    "df['education_level'] = df[isced_var].map(education_mapping).fillna('Missing/Inapplicable')\n",
    "education_categories = ['Low_Education', 'Medium_Education', 'High_Education', 'Missing/Inapplicable']\n",
    "df['education_level'] = pd.Categorical(df['education_level'], categories=education_categories, ordered=True)\n",
    "\n",
    "print(\"--- Feature Engineering B: Education Level (Ordered) Created ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# B.1. Feature Engineering: Vocational Qualification Indicator\n",
    "# Goal: Create a binary indicator if the individual holds any BTEC qualification.\n",
    "# ==============================================================================\n",
    "\n",
    "vocational_vars = ['n_btec1', 'n_btec2', 'n_btec3', 'n_btec4']\n",
    "\n",
    "# 1. Clean and ensure float type for calculation (already handled by initial cleaning, but re-assert)\n",
    "df = clean_missing_codes(df, vocational_vars)\n",
    "\n",
    "# 2. Identify Missingness: Check if all BTEC variables are NaN (Inapplicable/Missing)\n",
    "df['all_btec_nan'] = df[vocational_vars].isnull().all(axis=1)\n",
    "\n",
    "# 3. Create the binary indicator 'has_vocational_qual'\n",
    "# Calculate the maximum value across the 4 BTEC columns. If any is 1.0 (Mentioned), the max is 1.0.\n",
    "df['has_vocational_qual'] = df[vocational_vars].max(axis=1, skipna=True)\n",
    "\n",
    "# 4. Handle Missingness: If all BTEC variables were missing, set the new indicator to NaN.\n",
    "df.loc[df['all_btec_nan'], 'has_vocational_qual'] = np.nan\n",
    "\n",
    "# 5. Convert to nullable integer\n",
    "df = clean_and_convert_to_nullable_int(df, ['has_vocational_qual'])\n",
    "\n",
    "# 6. Clean up auxiliary column\n",
    "df = df.drop(columns=['all_btec_nan'])\n",
    "\n",
    "print(\"\\n--- Feature Engineering B.1: Vocational Qualification Indicator Completed ---\")\n",
    "print(df['has_vocational_qual'].value_counts(dropna=False))\n",
    "\n",
    "# ==============================================================================\n",
    "# C. Control Variables: Binary/Ordinal/Dummy Encoding - CORRECTED\n",
    "# ==============================================================================\n",
    "\n",
    "# Note: We assume core_categorical_vars cleaning (including n_sex, n_health, n_finnow) \n",
    "# to Int64 has already been executed successfully.\n",
    "\n",
    "# C.1. Binary Variables (from codes)\n",
    "\n",
    "# --- CORRECTED CODE FOR 'female' ---\n",
    "# 1. Initialize the column with 0 (Male)\n",
    "df['female'] = 0 \n",
    "# 2. Use .loc to set the '2' (Female) code to 1\n",
    "df.loc[df['n_sex'] == 2, 'female'] = 1\n",
    "# 3. Use .loc to set rows where 'n_sex' is missing (NA) to NaN\n",
    "df.loc[df['n_sex'].isna(), 'female'] = np.nan\n",
    "df['female'] = df['female'].astype('Int64') # Convert final result to Nullable Int\n",
    "\n",
    "# --- CORRECTED CODE FOR 'has_disability' ---\n",
    "# n_health: 1=Yes, 2=No. Recode to 'has_disability' (1=Yes, 0=No).\n",
    "# 1. Initialize the column with 0 (No)\n",
    "df['has_disability'] = 0\n",
    "# 2. Use .loc to set the '1' (Yes) code to 1\n",
    "df.loc[df['n_health'] == 1, 'has_disability'] = 1\n",
    "# 3. Use .loc to set rows where 'n_health' is missing (NA) to NaN\n",
    "df.loc[df['n_health'].isna(), 'has_disability'] = np.nan\n",
    "df['has_disability'] = df['has_disability'].astype('Int64')\n",
    "\n",
    "# --- CORRECTED CODE FOR 'financial_difficulty' ---\n",
    "# n_finnow: 4.0=Quite difficult, 5.0=Very difficult. Recode to 1=Difficulty, 0=No Difficulty.\n",
    "# 1. Initialize the column with 0 (No Difficulty)\n",
    "df['financial_difficulty'] = 0\n",
    "# 2. Use .loc to set the difficulty codes (4, 5) to 1\n",
    "df.loc[df['n_finnow'].isin([4, 5]), 'financial_difficulty'] = 1\n",
    "# 3. Use .loc to set rows where 'n_finnow' is missing (NA) to NaN\n",
    "df.loc[df['n_finnow'].isna(), 'financial_difficulty'] = np.nan\n",
    "df['financial_difficulty'] = df['financial_difficulty'].astype('Int64')\n",
    "\n",
    "\n",
    "# C.2. Categorical Variables: One-Hot Encoding (OHE)\n",
    "\n",
    "# Marital Status (n_marstat)\n",
    "df['n_marstat'] = df['n_marstat'].astype('category')\n",
    "marstat_dummies = pd.get_dummies(df['n_marstat'], prefix='marstat', dummy_na=True, drop_first=False)\n",
    "df = pd.concat([df, marstat_dummies], axis=1)\n",
    "\n",
    "# Ethnicity (n_racel_dv)\n",
    "df['n_racel_dv'] = df['n_racel_dv'].astype('category')\n",
    "racel_dummies = pd.get_dummies(df['n_racel_dv'], prefix='race', dummy_na=True, drop_first=False)\n",
    "df = pd.concat([df, racel_dummies], axis=1)\n",
    "\n",
    "print(\"--- Feature Engineering C: Control Variables (Binary/OHE) Completed ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# D. Advanced Employment Features: Status and Duration - (FIXED: Replacing np.where)\n",
    "# ==============================================================================\n",
    "\n",
    "# D.1. Current Unemployment Status (From n_jbstat)\n",
    "\n",
    "# 1. Binary Indicator: Currently Unemployed\n",
    "df['is_currently_unemployed'] = 0\n",
    "# Use .loc for assignment based on Int64 column\n",
    "df.loc[df['n_jbstat'] == 3, 'is_currently_unemployed'] = 1\n",
    "# Preserve missingness\n",
    "df.loc[df['n_jbstat'].isna(), 'is_currently_unemployed'] = np.nan\n",
    "df['is_currently_unemployed'] = df['is_currently_unemployed'].astype('Int64')\n",
    "\n",
    "\n",
    "# D.2. Key Non-Working/Inactive Indicators (Control Variables)\n",
    "\n",
    "# D.2.1. Long-Term Sick/Disabled (Value 8.0)\n",
    "df['inactive_lt_sick'] = 0\n",
    "df.loc[df['n_jbstat'] == 8, 'inactive_lt_sick'] = 1\n",
    "df.loc[df['n_jbstat'].isna(), 'inactive_lt_sick'] = np.nan\n",
    "df['inactive_lt_sick'] = df['inactive_lt_sick'].astype('Int64')\n",
    "\n",
    "# D.2.2. Looking After Family/Home (Value 6.0)\n",
    "df['inactive_home_family'] = 0\n",
    "df.loc[df['n_jbstat'] == 6, 'inactive_home_family'] = 1\n",
    "df.loc[df['n_jbstat'].isna(), 'inactive_home_family'] = np.nan\n",
    "df['inactive_home_family'] = df['inactive_home_family'].astype('Int64')\n",
    "\n",
    "\n",
    "# D.3. Unemployment Duration (Months)\n",
    "# Note: Duration calculation used only arithmetic, which is safe from this error.\n",
    "\n",
    "date_vars = ['n_jlendm', 'n_jlendy', 'n_ienddatm', 'n_ienddaty']\n",
    "# We assume date_vars were cleaned to Int64 earlier\n",
    "# No change needed for this block, as it handles NA correctly with .notna() and np.where(..., np.nan)\n",
    "# We re-run it here for completeness:\n",
    "df['last_unemployment_duration_months'] = np.where(\n",
    "    df['n_jlendy'].notna() & df['n_ienddaty'].notna() & df['n_jlendm'].notna() & df['n_ienddatm'].notna(),\n",
    "    (df['n_ienddaty'] - df['n_jlendy']) * 12 + (df['n_ienddatm'] - df['n_jlendm']),\n",
    "    np.nan\n",
    ")\n",
    "df.loc[df['n_jbstat'].isin([1, 2]), 'last_unemployment_duration_months'] = np.nan\n",
    "\n",
    "\n",
    "# D.4. Job Search Intensity and Motivation (Auxiliary Indicators)\n",
    "\n",
    "aux_employment_vars = ['n_jbhas', 'n_julk4wk', 'n_julkjb', 'n_jubgn', 'n_jbhad']\n",
    "# We assume aux_employment_vars were cleaned to Int64 earlier\n",
    "\n",
    "# D.4.1. Actively Seeking (Core ILO definition for the unemployed)\n",
    "# n_julk4wk=1 (Yes, looked for work)\n",
    "df['actively_seeking'] = 0\n",
    "df.loc[df['n_julk4wk'] == 1, 'actively_seeking'] = 1\n",
    "df.loc[df['n_julk4wk'].isna(), 'actively_seeking'] = np.nan\n",
    "df['actively_seeking'] = df['actively_seeking'].astype('Int64')\n",
    "\n",
    "# D.4.2. Marginal Attachment/Discouraged Worker \n",
    "# Condition: Did no paid work last week (n_jbhas=2) AND would like a job (n_julkjb=1)\n",
    "df['marginally_attached'] = 0\n",
    "df.loc[(df['n_jbhas'] == 2) & (df['n_julkjb'] == 1), 'marginally_attached'] = 1\n",
    "# Identify missingness where either component is NA\n",
    "missing_mask = df['n_jbhas'].isna() | df['n_julkjb'].isna()\n",
    "df.loc[missing_mask, 'marginally_attached'] = np.nan\n",
    "df['marginally_attached'] = df['marginally_attached'].astype('Int64')\n",
    "\n",
    "\n",
    "# D.4.3. Subjective Employment Chance (n_eprosh)\n",
    "# Recode subjective chance to be predictive (1=likely, 0=unlikely)\n",
    "# 1/2 = Likely, 3/4 = Unlikely\n",
    "df['subjective_job_chance_likely'] = 0\n",
    "df.loc[df['n_eprosh'].isin([1, 2]), 'subjective_job_chance_likely'] = 1\n",
    "df.loc[df['n_eprosh'].isna(), 'subjective_job_chance_likely'] = np.nan\n",
    "df['subjective_job_chance_likely'] = df['subjective_job_chance_likely'].astype('Int64')\n",
    "\n",
    "print(\"\\n--- Feature Engineering D: Advanced Employment Features (Fixed) Completed ---\")\n",
    "print(df[['is_currently_unemployed', 'last_unemployment_duration_months', 'marginally_attached']].describe())\n",
    "\n",
    "# ==============================================================================\n",
    "# E. Demographic and Socioeconomic Control Variables (FIXED)\n",
    "# Goal: Finalize encoding for age, marital status (using n_marstat), ethnicity, and financial perception.\n",
    "# ==============================================================================\n",
    "\n",
    "# Variables to ensure initial cleaning/Int64 conversion:\n",
    "control_vars_to_clean = ['n_age_dv', 'n_health', 'n_marstat', 'n_racel_dv', \n",
    "                         'n_urban_dv', 'n_finnow', 'n_finfut']\n",
    "df = clean_and_convert_to_nullable_int(df, control_vars_to_clean) # Assuming this function is correctly defined\n",
    "\n",
    "# E.1. Continuous Variable: Age (n_age_dv)\n",
    "# Age is already cleaned as Int64.\n",
    "\n",
    "# E.2. Binary Variables (n_urban_dv)\n",
    "# n_urban_dv: 1=Urban, 2=Rural. Create 'is_urban' (1=Urban, 0=Rural).\n",
    "df['is_urban'] = 0\n",
    "df.loc[df['n_urban_dv'] == 1, 'is_urban'] = 1\n",
    "df.loc[df['n_urban_dv'].isna(), 'is_urban'] = np.nan\n",
    "df['is_urban'] = df['is_urban'].astype('Int64')\n",
    "\n",
    "# E.3. Ordinal/Binary Financial Variables \n",
    "# n_finnow -> financial_difficulty (already handled in previous steps using .loc)\n",
    "\n",
    "# n_finfut: Subjective financial situation - future (1=Better, 2=Worse, 3=Same)\n",
    "df['n_finfut'] = df['n_finfut'].astype('category')\n",
    "finfut_dummies = pd.get_dummies(df['n_finfut'], prefix='finfut', dummy_na=True, drop_first=False)\n",
    "df = pd.concat([df, finfut_dummies], axis=1)\n",
    "\n",
    "\n",
    "# E.4. High-Dimensional Categorical Variable: Ethnicity (n_racel_dv)\n",
    "# Goal: Group 17 detailed codes into broader, more statistically meaningful categories (White, Asian, Black, etc.).\n",
    "\n",
    "# Define the grouping map based on standard UK research practice:\n",
    "ethnic_group_map = {\n",
    "    1: 'White', 2: 'White', 3: 'White', 4: 'White',\n",
    "    5: 'Mixed', 6: 'Mixed', 7: 'Mixed', 8: 'Mixed',\n",
    "    9: 'Asian', 10: 'Asian', 11: 'Asian', 12: 'Asian', 13: 'Asian',\n",
    "    14: 'Black', 15: 'Black', 16: 'Black',\n",
    "    17: 'Other', 97: 'Other'\n",
    "}\n",
    "\n",
    "df['ethnic_group'] = df['n_racel_dv'].map(ethnic_group_map)\n",
    "\n",
    "# Apply One-Hot Encoding to the grouped variable\n",
    "df['ethnic_group'] = df['ethnic_group'].astype('category')\n",
    "ethnic_dummies = pd.get_dummies(df['ethnic_group'], prefix='race_grp', dummy_na=True, drop_first=False)\n",
    "df = pd.concat([df, ethnic_dummies], axis=1)\n",
    "\n",
    "\n",
    "# E.5. Categorical Variable: Marital Status (n_marstat) - FIXED\n",
    "marital_var_name = 'n_marstat' # Use the correct column name\n",
    "\n",
    "# 1. Handle the 'Under 16 years' code (0.0) if present, converting it to NaN/NA.\n",
    "# This ensures that non-adults are excluded from the adult marital status analysis.\n",
    "df[marital_var_name] = df[marital_var_name].replace({0.0: np.nan})\n",
    "\n",
    "# 2. Apply One-Hot Encoding\n",
    "df[marital_var_name] = df[marital_var_name].astype('category')\n",
    "marstat_dummies_final = pd.get_dummies(df[marital_var_name], prefix='marstat_final', dummy_na=True, drop_first=False)\n",
    "df = pd.concat([df, marstat_dummies_final], axis=1)\n",
    "\n",
    "\n",
    "print(\"\\n--- Feature Engineering E: Demographic and Control Variables (Fixed) Completed ---\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Feature Engineering E: Demographic and Control Variables Completed ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Final Data Prep and Save (Complete and Optimized)\n",
    "# Goal: Keep only final engineered features and essential IDs/controls.\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. List of ALL desired final columns\n",
    "columns_to_keep = [\n",
    "    # 1. IDs/Raw Continuous/Counters \n",
    "    'pidp', 'n_age_dv', 'n_hhsize', 'n_nchild_dv', 'unemployment_spells_count', \n",
    "    \n",
    "    # 2. Outcomes\n",
    "    'mental_health_status', 'ghq12_continuous_score',\n",
    "    \n",
    "    # 3. Core Predictors (Categorical/Ordinal)\n",
    "    'early_career_unemp_risk', 'employment_stability_level', 'education_level',\n",
    "    \n",
    "    # 4. Employment Features (Binary/Continuous) - Step D\n",
    "    'is_currently_unemployed', 'last_unemployment_duration_months', \n",
    "    'inactive_lt_sick', 'inactive_home_family',\n",
    "    'actively_seeking', 'marginally_attached', 'subjective_job_chance_likely',\n",
    "    \n",
    "    # 5. Control Variables (Binary/Simple) - Steps C & E\n",
    "    'female', 'has_disability', 'financial_difficulty', 'is_urban',\n",
    "    'has_vocational_qual', \n",
    "]\n",
    "\n",
    "# 6. Add all generated dummy columns (One-Hot Encoded variables)\n",
    "# Collect all prefixes used for OHE variables across all steps\n",
    "dummy_prefixes = (\n",
    "    'marstat_final_', # Final Marital Status (n_marstat_dv fix)\n",
    "    'race_grp_',      # Ethnic Grouping (Step E)\n",
    "    'finfut_',        # Future Financial Status (Step E)\n",
    "    'marstat_',       # Old Marital Status (if any)\n",
    "    'race_',          # Old Ethnicity (if any)\n",
    ")\n",
    "\n",
    "# Dynamically capture all columns starting with the specified prefixes\n",
    "dummy_cols = [col for col in df.columns if col.startswith(dummy_prefixes)]\n",
    "\n",
    "# Combine and ensure final columns only include those existing in the DataFrame\n",
    "final_columns = list(set(columns_to_keep + dummy_cols))\n",
    "final_columns = [col for col in final_columns if col in df.columns]\n",
    "\n",
    "# Filter the DataFrame to keep only the final model columns\n",
    "df_final = df[final_columns].copy()\n",
    "\n",
    "\n",
    "# Save the optimized file\n",
    "output_file_name = \"n_indresp_FINAL_FEATURES_OPTIMIZED.csv\"\n",
    "output_file_path = os.path.join(processed_dir, output_file_name)\n",
    "df_final.to_csv(output_file_path, index=False)\n",
    "print(f\"\\n--- ALL Feature Engineering Stages Complete. Optimized data saved to: {output_file_path} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89321820-7d7c-4cf1-add1-5b33af0fdb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310fea2-a7f7-403d-9cff-be39c2afa01a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d50621-7f6b-48cd-9249-a8dd89999788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
